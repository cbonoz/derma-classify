{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f606c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "085a0c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Environment variables loaded from .env file\n",
      "Directories created: images/, metadata/\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import praw\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create directories for output\n",
    "os.makedirs('images', exist_ok=True)\n",
    "os.makedirs('metadata', exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Environment variables loaded from .env file\")\n",
    "print(\"Directories created: images/, metadata/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ccd7ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded credentials - Client ID: QH_iPhx9pf...\n",
      "Reddit API connection successful!\n",
      "Read-only mode: True\n"
     ]
    }
   ],
   "source": [
    "# Reddit API Configuration\n",
    "# Credentials are loaded from .env file\n",
    "\n",
    "# Load credentials from environment variables\n",
    "REDDIT_CLIENT_ID = os.getenv('REDDIT_CLIENT_ID')\n",
    "REDDIT_CLIENT_SECRET = os.getenv('REDDIT_CLIENT_SECRET')\n",
    "REDDIT_USER_AGENT = \"PersonalApp/1.0 by reupped\"\n",
    "\n",
    "# Check if credentials are loaded\n",
    "if not REDDIT_CLIENT_ID or not REDDIT_CLIENT_SECRET:\n",
    "    print(\"Error: Reddit API credentials not found in .env file\")\n",
    "    print(\"Please make sure your .env file contains:\")\n",
    "    print(\"REDDIT_CLIENT_ID=your_client_id\")\n",
    "    print(\"REDDIT_CLIENT_SECRET=your_client_secret\")\n",
    "else:\n",
    "    print(f\"Loaded credentials - Client ID: {REDDIT_CLIENT_ID[:10]}...\")\n",
    "\n",
    "# Initialize Reddit instance\n",
    "try:\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=REDDIT_CLIENT_ID,\n",
    "        client_secret=REDDIT_CLIENT_SECRET,\n",
    "        user_agent=REDDIT_USER_AGENT\n",
    "    )\n",
    "\n",
    "    # Test the connection\n",
    "    print(\"Reddit API connection successful!\")\n",
    "    print(f\"Read-only mode: {reddit.read_only}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Reddit API: {e}\")\n",
    "    print(\"Please check your Reddit API credentials in the .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dcc2398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing advanced bug detection:\n",
      "'What bit my kid?' -> unknown\n",
      "'Spider bite or something else?' -> spider\n",
      "'Mosquito bite on my arm' -> mosquito\n",
      "'Bed bug infestation help!' -> bed_bug\n",
      "'Can anyone help me ID these bug bites?' -> unknown\n",
      "'What is biting my son?' -> unknown\n",
      "'Three bites in a row on my arm' -> mosquito\n",
      "'Small red bumps on ankles after being outside' -> flea\n",
      "'Painful bite with two puncture marks' -> spider\n",
      "\n",
      "Advanced bug type detection system ready!\n"
     ]
    }
   ],
   "source": [
    "# Advanced bug type detection and classification system\n",
    "BUG_TYPES = {\n",
    "    'mosquito': ['mosquito', 'mosquitoes', 'skeeter', 'skeeters', 'mozzie', 'mozzies'],\n",
    "    'spider': ['spider', 'spiders', 'arachnid', 'black widow', 'brown recluse', 'wolf spider', 'house spider'],\n",
    "    'tick': ['tick', 'ticks', 'lyme', 'deer tick', 'wood tick', 'lone star tick'],\n",
    "    'flea': ['flea', 'fleas', 'flea bite', 'flea bites', 'cat flea', 'dog flea'],\n",
    "    'bed_bug': ['bed bug', 'bedbug', 'bed bugs', 'bedbugs', 'bed-bug', 'bed-bugs'],\n",
    "    'ant': ['ant', 'ants', 'fire ant', 'carpenter ant', 'red ant', 'black ant'],\n",
    "    'bee': ['bee', 'bees', 'honey bee', 'bumble bee', 'wasp', 'hornet', 'yellow jacket'],\n",
    "    'fly': ['fly', 'flies', 'horse fly', 'deer fly', 'black fly', 'sand fly', 'biting fly'],\n",
    "    'mite': ['mite', 'mites', 'chigger', 'chiggers', 'dust mite', 'scabies'],\n",
    "    'gnat': ['gnat', 'gnats', 'no-see-um', 'biting midge']\n",
    "}\n",
    "\n",
    "# Contextual clues that might help identify bug types\n",
    "CONTEXTUAL_CLUES = {\n",
    "    'mosquito': ['itchy', 'raised', 'red bump', 'welts', 'summer', 'evening', 'outdoors', 'water nearby'],\n",
    "    'spider': ['two puncture', 'fang marks', 'necrotic', 'dark center', 'spreading', 'painful'],\n",
    "    'tick': ['bullseye', 'bulls eye', 'circular', 'rash', 'lyme', 'woods', 'hiking', 'attached'],\n",
    "    'flea': ['ankles', 'lower legs', 'clusters', 'small red', 'pets', 'cat', 'dog'],\n",
    "    'bed_bug': ['linear', 'line', 'breakfast lunch dinner', 'bed', 'mattress', 'multiple', 'arms', 'back'],\n",
    "    'bee': ['stinger', 'swollen', 'allergic', 'painful', 'immediate', 'wasp'],\n",
    "    'ant': ['burning', 'fire', 'pustule', 'white head', 'yard', 'mound'],\n",
    "    'mite': ['extremely itchy', 'burrow', 'between fingers', 'scabies', 'chigger'],\n",
    "    'fly': ['painful', 'horse', 'deer', 'immediate pain', 'bleeding']\n",
    "}\n",
    "\n",
    "# Seasonal patterns\n",
    "SEASONAL_PATTERNS = {\n",
    "    'mosquito': ['summer', 'spring', 'warm', 'humid'],\n",
    "    'tick': ['spring', 'summer', 'fall', 'hiking season'],\n",
    "    'flea': ['year round', 'indoor'],\n",
    "    'bed_bug': ['year round', 'indoor'],\n",
    "    'bee': ['spring', 'summer', 'warm weather'],\n",
    "    'fly': ['summer', 'warm'],\n",
    "    'mite': ['year round']\n",
    "}\n",
    "\n",
    "def detect_bug_type_advanced(text, comments_text=\"\", debug=False):\n",
    "    \"\"\"\n",
    "    Advanced bug type detection using multiple strategies\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    comments_text = comments_text.lower()\n",
    "    combined_text = f\"{text} {comments_text}\"\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Analyzing text: '{text[:100]}...'\")\n",
    "        if comments_text:\n",
    "            print(f\"Comments: '{comments_text[:100]}...'\")\n",
    "\n",
    "    # Strategy 1: Direct keyword matching\n",
    "    keyword_scores = defaultdict(int)\n",
    "    found_keywords = defaultdict(list)\n",
    "\n",
    "    for bug_type, keywords in BUG_TYPES.items():\n",
    "        for keyword in keywords:\n",
    "            pattern = r'\\b' + re.escape(keyword) + r'\\b'\n",
    "            matches = len(re.findall(pattern, combined_text))\n",
    "            if matches > 0:\n",
    "                keyword_scores[bug_type] += matches * 3  # High weight for direct mentions\n",
    "                found_keywords[bug_type].append(f\"{keyword}({matches})\")\n",
    "\n",
    "    # Strategy 2: Contextual clue analysis\n",
    "    context_scores = defaultdict(int)\n",
    "    found_context = defaultdict(list)\n",
    "\n",
    "    for bug_type, clues in CONTEXTUAL_CLUES.items():\n",
    "        for clue in clues:\n",
    "            if clue in combined_text:\n",
    "                context_scores[bug_type] += 1\n",
    "                found_context[bug_type].append(clue)\n",
    "\n",
    "    # Strategy 3: Pattern-based detection for common scenarios\n",
    "    pattern_scores = defaultdict(int)\n",
    "    found_patterns = defaultdict(list)\n",
    "\n",
    "    # Common bite patterns\n",
    "    if any(phrase in combined_text for phrase in ['line of bites', 'three in a row', 'breakfast lunch dinner']):\n",
    "        pattern_scores['bed_bug'] += 2\n",
    "        found_patterns['bed_bug'].append('linear pattern')\n",
    "\n",
    "    if any(phrase in combined_text for phrase in ['cluster', 'grouped', 'multiple small']):\n",
    "        pattern_scores['flea'] += 1\n",
    "        found_patterns['flea'].append('clustered bites')\n",
    "\n",
    "    if any(phrase in combined_text for phrase in ['bullseye', 'bulls eye', 'expanding', 'circular rash']):\n",
    "        pattern_scores['tick'] += 3\n",
    "        found_patterns['tick'].append('bullseye pattern')\n",
    "\n",
    "    if any(phrase in combined_text for phrase in ['two holes', 'fang marks', 'puncture']):\n",
    "        pattern_scores['spider'] += 2\n",
    "        found_patterns['spider'].append('puncture marks')\n",
    "\n",
    "    # Strategy 4: Location-based hints\n",
    "    location_scores = defaultdict(int)\n",
    "    found_locations = defaultdict(list)\n",
    "\n",
    "    location_hints = {\n",
    "        'bed_bug': ['bed', 'mattress', 'hotel', 'couch', 'furniture'],\n",
    "        'flea': ['ankle', 'lower leg', 'sock line', 'pet'],\n",
    "        'mosquito': ['arm', 'leg', 'exposed skin', 'outside', 'evening'],\n",
    "        'tick': ['hairline', 'scalp', 'armpit', 'groin', 'hiking', 'woods'],\n",
    "        'spider': ['corner', 'basement', 'garage', 'closet', 'undisturbed area']\n",
    "    }\n",
    "\n",
    "    for bug_type, locations in location_hints.items():\n",
    "        for location in locations:\n",
    "            if location in combined_text:\n",
    "                location_scores[bug_type] += 1\n",
    "                found_locations[bug_type].append(location)\n",
    "\n",
    "    # Combine all scores\n",
    "    total_scores = defaultdict(int)\n",
    "    for bug_type in BUG_TYPES.keys():\n",
    "        total_scores[bug_type] = (\n",
    "            keyword_scores[bug_type] +\n",
    "            context_scores[bug_type] +\n",
    "            pattern_scores[bug_type] +\n",
    "            location_scores[bug_type]\n",
    "        )\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\nScoring breakdown:\")\n",
    "        for bug_type in BUG_TYPES.keys():\n",
    "            if total_scores[bug_type] > 0:\n",
    "                print(f\"  {bug_type}: {total_scores[bug_type]} points\")\n",
    "                if found_keywords[bug_type]:\n",
    "                    print(f\"    Keywords: {', '.join(found_keywords[bug_type])}\")\n",
    "                if found_context[bug_type]:\n",
    "                    print(f\"    Context: {', '.join(found_context[bug_type])}\")\n",
    "                if found_patterns[bug_type]:\n",
    "                    print(f\"    Patterns: {', '.join(found_patterns[bug_type])}\")\n",
    "                if found_locations[bug_type]:\n",
    "                    print(f\"    Locations: {', '.join(found_locations[bug_type])}\")\n",
    "\n",
    "    # Return best match or use fallback strategy\n",
    "    if total_scores:\n",
    "        best_match = max(total_scores.items(), key=lambda x: x[1])\n",
    "        if best_match[1] > 0:\n",
    "            if debug:\n",
    "                print(f\"Best match: {best_match[0]} (score: {best_match[1]})\")\n",
    "            return best_match[0]\n",
    "\n",
    "    # Fallback strategy: Use simple heuristics based on common post patterns\n",
    "    fallback_result = fallback_classification(combined_text, debug)\n",
    "    if debug:\n",
    "        print(f\"Using fallback classification: {fallback_result}\")\n",
    "\n",
    "    return fallback_result\n",
    "\n",
    "def fallback_classification(text, debug=False):\n",
    "    \"\"\"\n",
    "    Fallback classification for posts with minimal information\n",
    "    Uses heuristics based on common post characteristics\n",
    "    \"\"\"\n",
    "    # If it's asking for identification and has minimal context,\n",
    "    # try to infer from post characteristics\n",
    "\n",
    "    if any(phrase in text for phrase in ['what bit', 'what is this', 'help id', 'identify']):\n",
    "        # These are identification requests - try to infer from any available context\n",
    "\n",
    "        # Look for size/appearance clues\n",
    "        if any(word in text for word in ['small', 'tiny', 'little']):\n",
    "            if any(word in text for word in ['red', 'bump', 'itchy']):\n",
    "                return 'mosquito'  # Small, red, itchy = likely mosquito\n",
    "\n",
    "        # Look for location clues\n",
    "        if any(word in text for word in ['bed', 'night', 'morning']):\n",
    "            return 'bed_bug'\n",
    "\n",
    "        if any(word in text for word in ['ankle', 'leg', 'pet']):\n",
    "            return 'flea'\n",
    "\n",
    "        # Look for severity clues\n",
    "        if any(word in text for word in ['painful', 'swollen', 'severe']):\n",
    "            return 'spider'  # More severe reactions often spiders\n",
    "\n",
    "        # If mentioned outdoors/nature\n",
    "        if any(word in text for word in ['outside', 'yard', 'woods', 'hiking']):\n",
    "            return 'tick'\n",
    "\n",
    "    # Default for truly ambiguous cases\n",
    "    return 'unknown'\n",
    "\n",
    "# Update the main function for backward compatibility\n",
    "detect_bug_type = detect_bug_type_advanced\n",
    "\n",
    "def reclassify_existing_data():\n",
    "    \"\"\"\n",
    "    Reclassify all existing data with the new improved algorithm\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open('metadata/scraping_results.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        print(f\"Reclassifying {len(data)} existing entries...\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        reclassified_count = 0\n",
    "        type_changes = defaultdict(int)\n",
    "\n",
    "        for item in data:\n",
    "            old_type = item['bug_type']\n",
    "\n",
    "            # Use title and any available context for reclassification\n",
    "            new_type = detect_bug_type_advanced(\n",
    "                item['post_title'],\n",
    "                comments_text=\"\",  # We don't have comments stored\n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            if old_type != new_type:\n",
    "                item['bug_type'] = new_type\n",
    "                item['reclassified'] = True\n",
    "                reclassified_count += 1\n",
    "                type_changes[f\"{old_type} -> {new_type}\"] += 1\n",
    "\n",
    "        # Save the reclassified data\n",
    "        with open('metadata/scraping_results_reclassified.json', 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        print(f\"Reclassification complete!\")\n",
    "        print(f\"Changes made: {reclassified_count}/{len(data)} items\")\n",
    "        print(f\"Results saved to: metadata/scraping_results_reclassified.json\")\n",
    "\n",
    "        if type_changes:\n",
    "            print(\"\\nClassification changes:\")\n",
    "            for change, count in sorted(type_changes.items()):\n",
    "                print(f\"  {change}: {count} items\")\n",
    "\n",
    "        # Show new distribution\n",
    "        new_distribution = defaultdict(int)\n",
    "        for item in data:\n",
    "            new_distribution[item['bug_type']] += 1\n",
    "\n",
    "        print(f\"\\nNew bug type distribution:\")\n",
    "        for bug_type, count in sorted(new_distribution.items()):\n",
    "            percentage = (count / len(data)) * 100\n",
    "            print(f\"  {bug_type.upper()}: {count} images ({percentage:.1f}%)\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"No existing data found to reclassify\")\n",
    "        return None\n",
    "\n",
    "# Test the improved function\n",
    "test_texts = [\n",
    "    \"What bit my kid?\",\n",
    "    \"Spider bite or something else?\",\n",
    "    \"Mosquito bite on my arm\",\n",
    "    \"Bed bug infestation help!\",\n",
    "    \"Can anyone help me ID these bug bites?\",\n",
    "    \"What is biting my son?\",\n",
    "    \"Three bites in a row on my arm\",\n",
    "    \"Small red bumps on ankles after being outside\",\n",
    "    \"Painful bite with two puncture marks\"\n",
    "]\n",
    "\n",
    "print(\"Testing advanced bug detection:\")\n",
    "for text in test_texts:\n",
    "    bug_type = detect_bug_type_advanced(text, debug=False)\n",
    "    print(f\"'{text}' -> {bug_type}\")\n",
    "\n",
    "print(\"\\nAdvanced bug type detection system ready!\")\n",
    "\n",
    "# Reclassify existing data if available\n",
    "if os.path.exists('metadata/scraping_results.json'):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RECLASSIFYING EXISTING DATA\")\n",
    "    print(\"=\"*60)\n",
    "    reclassify_existing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f96079b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File renaming utility ready!\n"
     ]
    }
   ],
   "source": [
    "# File renaming utility based on reclassification\n",
    "def rename_files_by_classification():\n",
    "    \"\"\"\n",
    "    Rename existing downloaded files based on new classifications\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the reclassified data\n",
    "        reclassified_file = 'metadata/scraping_results_reclassified.json'\n",
    "        if not os.path.exists(reclassified_file):\n",
    "            print(\"No reclassified data found. Run reclassification first!\")\n",
    "            return\n",
    "\n",
    "        with open(reclassified_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Group by new bug type and create new counters\n",
    "        new_counters = defaultdict(int)\n",
    "        rename_mapping = []\n",
    "\n",
    "        # Sort data by bug type to ensure consistent numbering\n",
    "        data_by_type = defaultdict(list)\n",
    "        for item in data:\n",
    "            data_by_type[item['bug_type']].append(item)\n",
    "\n",
    "        # Create new filenames for each type\n",
    "        for bug_type in sorted(data_by_type.keys()):\n",
    "            items = data_by_type[bug_type]\n",
    "            for item in items:\n",
    "                old_filename = item['filename']\n",
    "                new_counters[bug_type] += 1\n",
    "                new_filename = f\"images/{bug_type.upper()}_{new_counters[bug_type]}.jpg\"\n",
    "\n",
    "                if old_filename != new_filename:\n",
    "                    rename_mapping.append((old_filename, new_filename))\n",
    "                    item['filename'] = new_filename  # Update metadata\n",
    "\n",
    "        # Perform the actual renaming\n",
    "        renamed_count = 0\n",
    "        for old_path, new_path in rename_mapping:\n",
    "            if os.path.exists(old_path):\n",
    "                # Ensure no conflict with existing files\n",
    "                if os.path.exists(new_path):\n",
    "                    # Create a temporary name to avoid conflicts\n",
    "                    temp_path = f\"{new_path}.temp\"\n",
    "                    os.rename(old_path, temp_path)\n",
    "                    old_path = temp_path\n",
    "\n",
    "                os.rename(old_path, new_path)\n",
    "                renamed_count += 1\n",
    "                print(f\"Renamed: {old_path} -> {new_path}\")\n",
    "\n",
    "        # Save updated metadata\n",
    "        with open(reclassified_file, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        print(f\"\\nFile renaming complete!\")\n",
    "        print(f\"Renamed {renamed_count} files\")\n",
    "        print(f\"Updated metadata saved to {reclassified_file}\")\n",
    "\n",
    "        # Show current file structure\n",
    "        print(f\"\\nCurrent images directory:\")\n",
    "        try:\n",
    "            images = sorted([f for f in os.listdir('images') if f.endswith('.jpg')])\n",
    "            type_counts = defaultdict(int)\n",
    "            for img in images:\n",
    "                bug_type = img.split('_')[0].lower()\n",
    "                type_counts[bug_type] += 1\n",
    "\n",
    "            for bug_type, count in sorted(type_counts.items()):\n",
    "                print(f\"  {bug_type.upper()}: {count} files\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(\"  No images directory found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during file renaming: {e}\")\n",
    "\n",
    "print(\"File renaming utility ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe1eeceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image processing functions with run management ready!\n"
     ]
    }
   ],
   "source": [
    "# Image download and processing functions with timestamped runs\n",
    "def create_run_directory():\n",
    "    \"\"\"Create a timestamped directory for this scraping run\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = f\"images/run{timestamp}\"\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Created run directory: {run_dir}\")\n",
    "    return run_dir, timestamp\n",
    "\n",
    "def is_image_url(url):\n",
    "    \"\"\"Check if URL points to an image\"\"\"\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp']\n",
    "    parsed_url = urlparse(url.lower())\n",
    "    return any(parsed_url.path.endswith(ext) for ext in image_extensions)\n",
    "\n",
    "def get_reddit_image_urls(submission):\n",
    "    \"\"\"Extract image URLs from a Reddit submission\"\"\"\n",
    "    urls = []\n",
    "\n",
    "    # Direct image link\n",
    "    if hasattr(submission, 'url') and is_image_url(submission.url):\n",
    "        urls.append(submission.url)\n",
    "\n",
    "    # Reddit gallery\n",
    "    if hasattr(submission, 'is_gallery') and submission.is_gallery:\n",
    "        try:\n",
    "            for item in submission.gallery_data['items']:\n",
    "                media_id = item['media_id']\n",
    "                if media_id in submission.media_metadata:\n",
    "                    media_info = submission.media_metadata[media_id]\n",
    "                    if 's' in media_info and 'u' in media_info['s']:\n",
    "                        # Convert preview URL to full resolution\n",
    "                        url = media_info['s']['u'].replace('preview.redd.it', 'i.redd.it')\n",
    "                        url = url.split('?')[0]  # Remove query parameters\n",
    "                        urls.append(url)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error processing gallery: {e}\")\n",
    "\n",
    "    # Check if it's an Imgur link\n",
    "    if 'imgur.com' in submission.url:\n",
    "        # Convert imgur links to direct image links\n",
    "        if '/a/' in submission.url or '/gallery/' in submission.url:\n",
    "            # Album/gallery - would need imgur API for full access\n",
    "            logger.info(f\"Imgur album detected: {submission.url}\")\n",
    "        else:\n",
    "            # Single image\n",
    "            imgur_id = submission.url.split('/')[-1].split('.')[0]\n",
    "            direct_url = f\"https://i.imgur.com/{imgur_id}.jpg\"\n",
    "            urls.append(direct_url)\n",
    "\n",
    "    return urls\n",
    "\n",
    "def download_image(url, filename):\n",
    "    \"\"\"Download an image from URL and save it\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Check if response is actually an image\n",
    "        content_type = response.headers.get('content-type', '')\n",
    "        if not content_type.startswith('image/'):\n",
    "            logger.warning(f\"URL doesn't return an image: {url}\")\n",
    "            return False\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        logger.info(f\"Downloaded: {filename}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error downloading {url}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Global variables for run management\n",
    "current_run_dir = None\n",
    "current_timestamp = None\n",
    "bug_counters = defaultdict(int)\n",
    "\n",
    "def initialize_new_run():\n",
    "    \"\"\"Initialize a new scraping run with timestamped directory\"\"\"\n",
    "    global current_run_dir, current_timestamp, bug_counters\n",
    "\n",
    "    current_run_dir, current_timestamp = create_run_directory()\n",
    "    bug_counters = defaultdict(int)  # Reset counters for new run\n",
    "\n",
    "    return current_run_dir, current_timestamp\n",
    "\n",
    "def get_next_filename(bug_type):\n",
    "    \"\"\"Get the next filename for a bug type in the current run\"\"\"\n",
    "    global current_run_dir, bug_counters\n",
    "\n",
    "    if current_run_dir is None:\n",
    "        # Initialize if not already done\n",
    "        initialize_new_run()\n",
    "\n",
    "    bug_counters[bug_type] += 1\n",
    "    return f\"{current_run_dir}/{bug_type.upper()}_{bug_counters[bug_type]}.jpg\"\n",
    "\n",
    "def get_run_summary():\n",
    "    \"\"\"Get summary of the current run\"\"\"\n",
    "    global current_run_dir, current_timestamp, bug_counters\n",
    "\n",
    "    if current_run_dir is None:\n",
    "        return \"No active run\"\n",
    "\n",
    "    summary = {\n",
    "        'run_directory': current_run_dir,\n",
    "        'timestamp': current_timestamp,\n",
    "        'bug_type_counts': dict(bug_counters),\n",
    "        'total_images': sum(bug_counters.values())\n",
    "    }\n",
    "\n",
    "    return summary\n",
    "\n",
    "print(\"Image processing functions with run management ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc82d657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main scraping function with run management ready!\n"
     ]
    }
   ],
   "source": [
    "# Main scraping function with run management\n",
    "def scrape_bugbites_subreddit(limit=50, time_filter='week'):\n",
    "    \"\"\"\n",
    "    Scrape r/bugbites subreddit for images and classify them\n",
    "    Each run gets its own timestamped directory\n",
    "\n",
    "    Args:\n",
    "        limit: Number of posts to scrape\n",
    "        time_filter: Time filter for posts ('hour', 'day', 'week', 'month', 'year', 'all')\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize new run\n",
    "    run_dir, timestamp = initialize_new_run()\n",
    "    scraped_data = []\n",
    "\n",
    "    print(f\"Starting new scraping run: {timestamp}\")\n",
    "    print(f\"Images will be saved to: {run_dir}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        subreddit = reddit.subreddit('bugbites')\n",
    "\n",
    "        # Get posts from the subreddit\n",
    "        posts = subreddit.hot(limit=limit)  # You can also use .new(), .top(time_filter=time_filter)\n",
    "\n",
    "        for post_count, submission in enumerate(posts, 1):\n",
    "            print(f\"\\n--- Processing post {post_count}/{limit} ---\")\n",
    "            print(f\"Title: {submission.title[:80]}...\")\n",
    "\n",
    "            # Analyze title and selftext for bug type\n",
    "            combined_text = f\"{submission.title} {submission.selftext}\"\n",
    "\n",
    "            # Also check comments for additional context\n",
    "            submission.comments.replace_more(limit=0)  # Don't load \"more comments\"\n",
    "            comments_text = \"\"\n",
    "            for comment in submission.comments.list()[:10]:  # First 10 comments\n",
    "                if hasattr(comment, 'body'):\n",
    "                    comments_text += f\" {comment.body}\"\n",
    "\n",
    "            # Use the advanced detection system\n",
    "            bug_type = detect_bug_type_advanced(combined_text, comments_text, debug=False)\n",
    "\n",
    "            print(f\"Detected bug type: {bug_type}\")\n",
    "\n",
    "            # Get image URLs\n",
    "            image_urls = get_reddit_image_urls(submission)\n",
    "\n",
    "            if image_urls:\n",
    "                print(f\"Found {len(image_urls)} image(s)\")\n",
    "\n",
    "                for img_url in image_urls:\n",
    "                    filename = get_next_filename(bug_type)\n",
    "\n",
    "                    if download_image(img_url, filename):\n",
    "                        # Store metadata with run information\n",
    "                        metadata = {\n",
    "                            'run_timestamp': timestamp,\n",
    "                            'run_directory': run_dir,\n",
    "                            'filename': filename,\n",
    "                            'bug_type': bug_type,\n",
    "                            'post_title': submission.title,\n",
    "                            'post_url': f\"https://reddit.com{submission.permalink}\",\n",
    "                            'image_url': img_url,\n",
    "                            'post_score': submission.score,\n",
    "                            'num_comments': submission.num_comments,\n",
    "                            'created_utc': submission.created_utc,\n",
    "                            'author': str(submission.author) if submission.author else '[deleted]',\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        }\n",
    "                        scraped_data.append(metadata)\n",
    "\n",
    "                        print(f\"Saved as: {filename}\")\n",
    "            else:\n",
    "                print(\"No images found in this post\")\n",
    "\n",
    "            # Be respectful to Reddit's API\n",
    "            time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error scraping subreddit: {e}\")\n",
    "\n",
    "    return scraped_data, timestamp\n",
    "\n",
    "def save_metadata_with_run(scraped_data, timestamp):\n",
    "    \"\"\"Save scraping metadata with run-specific information\"\"\"\n",
    "\n",
    "    # Save run-specific metadata\n",
    "    run_metadata_file = f'metadata/scraping_results_run{timestamp}.json'\n",
    "    with open(run_metadata_file, 'w') as f:\n",
    "        json.dump(scraped_data, f, indent=2)\n",
    "\n",
    "    # Also append to master metadata file\n",
    "    master_metadata_file = 'metadata/all_scraping_results.json'\n",
    "\n",
    "    # Load existing master data if it exists\n",
    "    all_data = []\n",
    "    if os.path.exists(master_metadata_file):\n",
    "        try:\n",
    "            with open(master_metadata_file, 'r') as f:\n",
    "                all_data = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            all_data = []\n",
    "\n",
    "    # Add new data\n",
    "    all_data.extend(scraped_data)\n",
    "\n",
    "    # Save updated master file\n",
    "    with open(master_metadata_file, 'w') as f:\n",
    "        json.dump(all_data, f, indent=2)\n",
    "\n",
    "    print(f\"Run metadata saved to: {run_metadata_file}\")\n",
    "    print(f\"Master metadata updated: {master_metadata_file}\")\n",
    "\n",
    "    # Save run summary\n",
    "    run_summary = get_run_summary()\n",
    "    run_summary['metadata_file'] = run_metadata_file\n",
    "    run_summary['total_posts_processed'] = len(scraped_data)\n",
    "\n",
    "    summary_file = f'metadata/run_summary_{timestamp}.json'\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(run_summary, f, indent=2)\n",
    "\n",
    "    print(f\"Run summary saved to: {summary_file}\")\n",
    "\n",
    "    return run_metadata_file, master_metadata_file\n",
    "\n",
    "def list_all_runs():\n",
    "    \"\"\"List all previous scraping runs\"\"\"\n",
    "    try:\n",
    "        runs = []\n",
    "\n",
    "        # Look for run directories\n",
    "        if os.path.exists('images'):\n",
    "            for item in os.listdir('images'):\n",
    "                if item.startswith('run') and os.path.isdir(f'images/{item}'):\n",
    "                    timestamp = item[3:]  # Remove 'run' prefix\n",
    "\n",
    "                    # Count files in directory\n",
    "                    run_dir = f'images/{item}'\n",
    "                    image_count = len([f for f in os.listdir(run_dir) if f.endswith('.jpg')])\n",
    "\n",
    "                    # Try to load summary if available\n",
    "                    summary_file = f'metadata/run_summary_{timestamp}.json'\n",
    "                    bug_counts = {}\n",
    "                    if os.path.exists(summary_file):\n",
    "                        with open(summary_file, 'r') as f:\n",
    "                            summary = json.load(f)\n",
    "                            bug_counts = summary.get('bug_type_counts', {})\n",
    "\n",
    "                    runs.append({\n",
    "                        'timestamp': timestamp,\n",
    "                        'directory': run_dir,\n",
    "                        'image_count': image_count,\n",
    "                        'bug_counts': bug_counts\n",
    "                    })\n",
    "\n",
    "        # Sort by timestamp\n",
    "        runs.sort(key=lambda x: x['timestamp'], reverse=True)\n",
    "\n",
    "        if runs:\n",
    "            print(f\"Found {len(runs)} previous runs:\")\n",
    "            print(\"-\" * 80)\n",
    "            for run in runs:\n",
    "                print(f\"Run {run['timestamp']}: {run['image_count']} images in {run['directory']}\")\n",
    "                if run['bug_counts']:\n",
    "                    for bug_type, count in run['bug_counts'].items():\n",
    "                        print(f\"  {bug_type.upper()}: {count}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"No previous runs found\")\n",
    "\n",
    "        return runs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing runs: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"Main scraping function with run management ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ae67062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new Reddit scraping run...\n",
      "Will scrape 10 posts from the past week\n",
      "============================================================\n",
      "\n",
      "Previous runs:\n",
      "Found 1 previous runs:\n",
      "--------------------------------------------------------------------------------\n",
      "Run 0: 19 images in images/run0\n",
      "\n",
      "\n",
      "============================================================\n",
      "STARTING NEW RUN\n",
      "============================================================\n",
      "Created run directory: images/run20250806_200212\n",
      "Starting new scraping run: 20250806_200212\n",
      "Images will be saved to: images/run20250806_200212\n",
      "============================================================\n",
      "\n",
      "--- Processing post 1/10 ---\n",
      "Title: Read this before posting....\n",
      "\n",
      "--- Processing post 1/10 ---\n",
      "Title: Read this before posting....\n",
      "Detected bug type: flea\n",
      "No images found in this post\n",
      "Detected bug type: flea\n",
      "No images found in this post\n",
      "\n",
      "--- Processing post 2/10 ---\n",
      "Title: What bit my kid?...\n",
      "Detected bug type: bed_bug\n",
      "Found 2 image(s)\n",
      "\n",
      "--- Processing post 2/10 ---\n",
      "Title: What bit my kid?...\n",
      "Detected bug type: bed_bug\n",
      "Found 2 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_200212/BED_BUG_1.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_200212/BED_BUG_2.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_200212/BED_BUG_2.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_200212/BED_BUG_1.jpg\n",
      "Saved as: images/run20250806_200212/BED_BUG_2.jpg\n",
      "\n",
      "--- Processing post 3/10 ---\n",
      "Title: How do I get rid of this?...\n",
      "Detected bug type: mosquito\n",
      "Found 1 image(s)\n",
      "\n",
      "--- Processing post 3/10 ---\n",
      "Title: How do I get rid of this?...\n",
      "Detected bug type: mosquito\n",
      "Found 1 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_200212/MOSQUITO_1.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_200212/MOSQUITO_1.jpg\n",
      "\n",
      "--- Processing post 4/10 ---\n",
      "Title: What got me ??? Texas...\n",
      "Detected bug type: mosquito\n",
      "Found 2 image(s)\n",
      "\n",
      "--- Processing post 4/10 ---\n",
      "Title: What got me ??? Texas...\n",
      "Detected bug type: mosquito\n",
      "Found 2 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_200212/MOSQUITO_2.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_200212/MOSQUITO_3.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_200212/MOSQUITO_3.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_200212/MOSQUITO_2.jpg\n",
      "Saved as: images/run20250806_200212/MOSQUITO_3.jpg\n",
      "\n",
      "--- Processing post 5/10 ---\n",
      "Title: Bug bites?? on dad...\n",
      "Detected bug type: mosquito\n",
      "Found 2 image(s)\n",
      "\n",
      "--- Processing post 5/10 ---\n",
      "Title: Bug bites?? on dad...\n",
      "Detected bug type: mosquito\n",
      "Found 2 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_200212/MOSQUITO_4.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_200212/MOSQUITO_5.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_200212/MOSQUITO_5.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_200212/MOSQUITO_4.jpg\n",
      "Saved as: images/run20250806_200212/MOSQUITO_5.jpg\n",
      "\n",
      "--- Processing post 6/10 ---\n",
      "Title: What is biting my son?...\n",
      "Detected bug type: unknown\n",
      "Found 8 image(s)\n",
      "\n",
      "--- Processing post 6/10 ---\n",
      "Title: What is biting my son?...\n",
      "Detected bug type: unknown\n",
      "Found 8 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_200212/UNKNOWN_1.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_200212/UNKNOWN_2.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_200212/UNKNOWN_2.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_200212/UNKNOWN_1.jpg\n",
      "Saved as: images/run20250806_200212/UNKNOWN_2.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_200212/UNKNOWN_3.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_200212/UNKNOWN_4.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_200212/UNKNOWN_4.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_200212/UNKNOWN_3.jpg\n",
      "Saved as: images/run20250806_200212/UNKNOWN_4.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_200212/UNKNOWN_5.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_200212/UNKNOWN_6.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_200212/UNKNOWN_6.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_200212/UNKNOWN_7.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_200212/UNKNOWN_7.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_200212/UNKNOWN_5.jpg\n",
      "Saved as: images/run20250806_200212/UNKNOWN_6.jpg\n",
      "Saved as: images/run20250806_200212/UNKNOWN_7.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_200212/UNKNOWN_8.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_200212/UNKNOWN_8.jpg\n",
      "\n",
      "--- Processing post 7/10 ---\n",
      "Title: Bug bite...\n",
      "Detected bug type: mosquito\n",
      "Found 1 image(s)\n",
      "\n",
      "--- Processing post 7/10 ---\n",
      "Title: Bug bite...\n",
      "Detected bug type: mosquito\n",
      "Found 1 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_200212/MOSQUITO_6.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_200212/MOSQUITO_6.jpg\n",
      "\n",
      "--- Processing post 8/10 ---\n",
      "Title: Can anyone help me ID these bug bites?...\n",
      "Detected bug type: bed_bug\n",
      "Found 1 image(s)\n",
      "\n",
      "--- Processing post 8/10 ---\n",
      "Title: Can anyone help me ID these bug bites?...\n",
      "Detected bug type: bed_bug\n",
      "Found 1 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_200212/BED_BUG_3.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_200212/BED_BUG_3.jpg\n",
      "\n",
      "--- Processing post 9/10 ---\n",
      "Title: What is this ?? Showed up a couple days ago with another small dot on my stomach...\n",
      "Detected bug type: mosquito\n",
      "Found 1 image(s)\n",
      "\n",
      "--- Processing post 9/10 ---\n",
      "Title: What is this ?? Showed up a couple days ago with another small dot on my stomach...\n",
      "Detected bug type: mosquito\n",
      "Found 1 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_200212/MOSQUITO_7.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_200212/MOSQUITO_7.jpg\n",
      "\n",
      "--- Processing post 10/10 ---\n",
      "Title: What the freak is this...\n",
      "Detected bug type: mosquito\n",
      "Found 1 image(s)\n",
      "\n",
      "--- Processing post 10/10 ---\n",
      "Title: What the freak is this...\n",
      "Detected bug type: mosquito\n",
      "Found 1 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_200212/MOSQUITO_8.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_200212/MOSQUITO_8.jpg\n",
      "Run metadata saved to: metadata/scraping_results_run20250806_200212.json\n",
      "Master metadata updated: metadata/all_scraping_results.json\n",
      "Run summary saved to: metadata/run_summary_20250806_200212.json\n",
      "\n",
      "============================================================\n",
      "SCRAPING RUN COMPLETE!\n",
      "============================================================\n",
      "Run timestamp: 20250806_200212\n",
      "Total images downloaded: 19\n",
      "\n",
      "Images by bug type (this run):\n",
      "  BED_BUG: 3 images\n",
      "  MOSQUITO: 8 images\n",
      "  UNKNOWN: 8 images\n",
      "\n",
      "Run directory: images/run20250806_200212\n",
      "Files created:\n",
      "  BED_BUG_1.jpg\n",
      "  BED_BUG_2.jpg\n",
      "  BED_BUG_3.jpg\n",
      "  MOSQUITO_1.jpg\n",
      "  MOSQUITO_2.jpg\n",
      "  MOSQUITO_3.jpg\n",
      "  MOSQUITO_4.jpg\n",
      "  MOSQUITO_5.jpg\n",
      "  MOSQUITO_6.jpg\n",
      "  MOSQUITO_7.jpg\n",
      "  ... and 9 more\n",
      "\n",
      "Metadata files:\n",
      "  Run-specific: metadata/scraping_results_run20250806_200212.json\n",
      "  Master file: metadata/all_scraping_results.json\n",
      "\n",
      "Run completed at: 2025-08-06 20:02:26\n",
      "Run metadata saved to: metadata/scraping_results_run20250806_200212.json\n",
      "Master metadata updated: metadata/all_scraping_results.json\n",
      "Run summary saved to: metadata/run_summary_20250806_200212.json\n",
      "\n",
      "============================================================\n",
      "SCRAPING RUN COMPLETE!\n",
      "============================================================\n",
      "Run timestamp: 20250806_200212\n",
      "Total images downloaded: 19\n",
      "\n",
      "Images by bug type (this run):\n",
      "  BED_BUG: 3 images\n",
      "  MOSQUITO: 8 images\n",
      "  UNKNOWN: 8 images\n",
      "\n",
      "Run directory: images/run20250806_200212\n",
      "Files created:\n",
      "  BED_BUG_1.jpg\n",
      "  BED_BUG_2.jpg\n",
      "  BED_BUG_3.jpg\n",
      "  MOSQUITO_1.jpg\n",
      "  MOSQUITO_2.jpg\n",
      "  MOSQUITO_3.jpg\n",
      "  MOSQUITO_4.jpg\n",
      "  MOSQUITO_5.jpg\n",
      "  MOSQUITO_6.jpg\n",
      "  MOSQUITO_7.jpg\n",
      "  ... and 9 more\n",
      "\n",
      "Metadata files:\n",
      "  Run-specific: metadata/scraping_results_run20250806_200212.json\n",
      "  Master file: metadata/all_scraping_results.json\n",
      "\n",
      "Run completed at: 2025-08-06 20:02:26\n"
     ]
    }
   ],
   "source": [
    "# Execute the scraping with timestamped runs\n",
    "# Note: Make sure you've updated the Reddit API credentials above before running this!\n",
    "\n",
    "# Configure scraping parameters\n",
    "POSTS_TO_SCRAPE = 10  # Start with a smaller number for testing\n",
    "TIME_FILTER = 'week'  # 'hour', 'day', 'week', 'month', 'year', 'all'\n",
    "\n",
    "print(\"Starting new Reddit scraping run...\")\n",
    "print(f\"Will scrape {POSTS_TO_SCRAPE} posts from the past {TIME_FILTER}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# List existing runs first\n",
    "print(\"\\nPrevious runs:\")\n",
    "list_all_runs()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STARTING NEW RUN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run the scraper with new timestamped system\n",
    "scraped_data, run_timestamp = scrape_bugbites_subreddit(limit=POSTS_TO_SCRAPE, time_filter=TIME_FILTER)\n",
    "\n",
    "# Save metadata with run information\n",
    "if scraped_data:\n",
    "    run_metadata_file, master_metadata_file = save_metadata_with_run(scraped_data, run_timestamp)\n",
    "\n",
    "    # Print detailed summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SCRAPING RUN COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Run timestamp: {run_timestamp}\")\n",
    "    print(f\"Total images downloaded: {len(scraped_data)}\")\n",
    "\n",
    "    # Count by bug type for this run\n",
    "    run_bug_counts = defaultdict(int)\n",
    "    for item in scraped_data:\n",
    "        run_bug_counts[item['bug_type']] += 1\n",
    "\n",
    "    print(f\"\\nImages by bug type (this run):\")\n",
    "    for bug_type, count in sorted(run_bug_counts.items()):\n",
    "        print(f\"  {bug_type.upper()}: {count} images\")\n",
    "\n",
    "    # Show run directory structure\n",
    "    print(f\"\\nRun directory: {current_run_dir}\")\n",
    "    try:\n",
    "        files = sorted([f for f in os.listdir(current_run_dir) if f.endswith('.jpg')])\n",
    "        print(f\"Files created:\")\n",
    "        for f in files[:10]:  # Show first 10 files\n",
    "            print(f\"  {f}\")\n",
    "        if len(files) > 10:\n",
    "            print(f\"  ... and {len(files) - 10} more\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(f\"\\nMetadata files:\")\n",
    "    print(f\"  Run-specific: {run_metadata_file}\")\n",
    "    print(f\"  Master file: {master_metadata_file}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo images were downloaded. Check your Reddit API credentials and internet connection.\")\n",
    "\n",
    "print(f\"\\nRun completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67f7b816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Management Utilities Ready!\n",
      "\n",
      "Current runs:\n",
      "Found 2 previous runs:\n",
      "--------------------------------------------------------------------------------\n",
      "Run 20250806_200212: 19 images in images/run20250806_200212\n",
      "  BED_BUG: 3\n",
      "  MOSQUITO: 8\n",
      "  UNKNOWN: 8\n",
      "\n",
      "Run 0: 19 images in images/run0\n",
      "\n",
      "\n",
      "Analyzing most recent run:\n",
      "Found 2 previous runs:\n",
      "--------------------------------------------------------------------------------\n",
      "Run 20250806_200212: 19 images in images/run20250806_200212\n",
      "  BED_BUG: 3\n",
      "  MOSQUITO: 8\n",
      "  UNKNOWN: 8\n",
      "\n",
      "Run 0: 19 images in images/run0\n",
      "\n",
      "Analyzing most recent run: 20250806_200212\n",
      "\n",
      "Analysis of run 20250806_200212:\n",
      "==================================================\n",
      "Total images: 19\n",
      "Run directory: images/run20250806_200212\n",
      "Scraped at: 2025-08-06T20:02:14.159615\n",
      "\n",
      "Bug Type Distribution:\n",
      "  MOSQUITO: 8 images (42.1%)\n",
      "  UNKNOWN: 8 images (42.1%)\n",
      "  BED_BUG: 3 images (15.8%)\n",
      "\n",
      "Average post score: 1.1\n",
      "Average comments per post: 1.0\n",
      "\n",
      "Sample classifications:\n",
      "  BED_BUG: What bit my kid?...\n",
      "  BED_BUG: What bit my kid?...\n",
      "  MOSQUITO: How do I get rid of this?...\n",
      "  MOSQUITO: What got me ??? Texas...\n",
      "  MOSQUITO: What got me ??? Texas...\n"
     ]
    }
   ],
   "source": [
    "# Run management and analysis utilities\n",
    "\n",
    "def analyze_run(timestamp=None):\n",
    "    \"\"\"Analyze a specific run or the most recent run\"\"\"\n",
    "\n",
    "    if timestamp is None:\n",
    "        # Find most recent run\n",
    "        runs = list_all_runs()\n",
    "        if not runs:\n",
    "            print(\"No runs found\")\n",
    "            return\n",
    "        timestamp = runs[0]['timestamp']\n",
    "        print(f\"Analyzing most recent run: {timestamp}\")\n",
    "\n",
    "    metadata_file = f'metadata/scraping_results_run{timestamp}.json'\n",
    "\n",
    "    try:\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        if not data:\n",
    "            print(\"No data found in run\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\nAnalysis of run {timestamp}:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Total images: {len(data)}\")\n",
    "        print(f\"Run directory: {data[0]['run_directory']}\")\n",
    "        print(f\"Scraped at: {data[0]['scraped_at']}\")\n",
    "\n",
    "        # Bug type distribution\n",
    "        bug_counts = defaultdict(int)\n",
    "        total_score = 0\n",
    "        total_comments = 0\n",
    "\n",
    "        for item in data:\n",
    "            bug_counts[item['bug_type']] += 1\n",
    "            total_score += item.get('post_score', 0)\n",
    "            total_comments += item.get('num_comments', 0)\n",
    "\n",
    "        print(f\"\\nBug Type Distribution:\")\n",
    "        for bug_type, count in sorted(bug_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            percentage = (count / len(data)) * 100\n",
    "            print(f\"  {bug_type.upper()}: {count} images ({percentage:.1f}%)\")\n",
    "\n",
    "        print(f\"\\nAverage post score: {total_score / len(data):.1f}\")\n",
    "        print(f\"Average comments per post: {total_comments / len(data):.1f}\")\n",
    "\n",
    "        # Show some sample classifications\n",
    "        print(f\"\\nSample classifications:\")\n",
    "        for i, item in enumerate(data[:5]):\n",
    "            print(f\"  {item['bug_type'].upper()}: {item['post_title'][:60]}...\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Run metadata not found: {metadata_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing run: {e}\")\n",
    "\n",
    "def compare_runs():\n",
    "    \"\"\"Compare all runs\"\"\"\n",
    "    runs = list_all_runs()\n",
    "\n",
    "    if len(runs) < 2:\n",
    "        print(\"Need at least 2 runs to compare\")\n",
    "        return\n",
    "\n",
    "    print(\"Run Comparison:\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Load all run data\n",
    "    all_run_data = []\n",
    "    for run in runs:\n",
    "        timestamp = run['timestamp']\n",
    "        metadata_file = f'metadata/scraping_results_run{timestamp}.json'\n",
    "\n",
    "        try:\n",
    "            with open(metadata_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            bug_counts = defaultdict(int)\n",
    "            for item in data:\n",
    "                bug_counts[item['bug_type']] += 1\n",
    "\n",
    "            all_run_data.append({\n",
    "                'timestamp': timestamp,\n",
    "                'total_images': len(data),\n",
    "                'bug_counts': dict(bug_counts)\n",
    "            })\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Print comparison table\n",
    "    print(f\"{'Run':<16} {'Total':<8} {'Unknown':<8} {'Mosquito':<10} {'Spider':<8} {'Others':<8}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for run_data in all_run_data:\n",
    "        timestamp = run_data['timestamp']\n",
    "        total = run_data['total_images']\n",
    "        unknown = run_data['bug_counts'].get('unknown', 0)\n",
    "        mosquito = run_data['bug_counts'].get('mosquito', 0)\n",
    "        spider = run_data['bug_counts'].get('spider', 0)\n",
    "        others = total - unknown - mosquito - spider\n",
    "\n",
    "        print(f\"{timestamp:<16} {total:<8} {unknown:<8} {mosquito:<10} {spider:<8} {others:<8}\")\n",
    "\n",
    "def cleanup_old_runs(keep_recent=5):\n",
    "    \"\"\"Clean up old runs, keeping only the most recent ones\"\"\"\n",
    "    runs = list_all_runs()\n",
    "\n",
    "    if len(runs) <= keep_recent:\n",
    "        print(f\"Only {len(runs)} runs found, nothing to clean up\")\n",
    "        return\n",
    "\n",
    "    runs_to_delete = runs[keep_recent:]\n",
    "\n",
    "    print(f\"Will delete {len(runs_to_delete)} old runs, keeping {keep_recent} most recent:\")\n",
    "    for run in runs_to_delete:\n",
    "        print(f\"  {run['timestamp']}\")\n",
    "\n",
    "    confirm = input(\"\\nProceed with deletion? (y/N): \")\n",
    "    if confirm.lower() != 'y':\n",
    "        print(\"Cleanup cancelled\")\n",
    "        return\n",
    "\n",
    "    deleted_count = 0\n",
    "    for run in runs_to_delete:\n",
    "        timestamp = run['timestamp']\n",
    "\n",
    "        # Delete run directory\n",
    "        run_dir = f\"images/run{timestamp}\"\n",
    "        if os.path.exists(run_dir):\n",
    "            import shutil\n",
    "            shutil.rmtree(run_dir)\n",
    "            deleted_count += 1\n",
    "\n",
    "        # Delete metadata files\n",
    "        for file_pattern in [f'metadata/scraping_results_run{timestamp}.json',\n",
    "                           f'metadata/run_summary_{timestamp}.json']:\n",
    "            if os.path.exists(file_pattern):\n",
    "                os.remove(file_pattern)\n",
    "\n",
    "    print(f\"Cleanup complete! Deleted {deleted_count} run directories\")\n",
    "\n",
    "# Quick analysis of existing data\n",
    "print(\"Run Management Utilities Ready!\")\n",
    "print(\"\\nCurrent runs:\")\n",
    "list_all_runs()\n",
    "\n",
    "if os.path.exists('images') and any(d.startswith('run') for d in os.listdir('images')):\n",
    "    print(\"\\nAnalyzing most recent run:\")\n",
    "    analyze_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e330c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No metadata file found yet. Run the scraper first!\n"
     ]
    }
   ],
   "source": [
    "# Analysis and utility functions\n",
    "def analyze_scraped_data(metadata_file='metadata/scraping_results.json'):\n",
    "    \"\"\"Analyze the scraped data and show statistics\"\"\"\n",
    "\n",
    "    try:\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        if not data:\n",
    "            print(\"No data found in metadata file\")\n",
    "            return\n",
    "\n",
    "        print(f\"Analysis of {len(data)} scraped images:\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        # Bug type distribution\n",
    "        bug_counts = defaultdict(int)\n",
    "        total_score = 0\n",
    "        total_comments = 0\n",
    "\n",
    "        for item in data:\n",
    "            bug_counts[item['bug_type']] += 1\n",
    "            total_score += item.get('post_score', 0)\n",
    "            total_comments += item.get('num_comments', 0)\n",
    "\n",
    "        print(\"\\nBug Type Distribution:\")\n",
    "        for bug_type, count in sorted(bug_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            percentage = (count / len(data)) * 100\n",
    "            print(f\"  {bug_type.upper()}: {count} images ({percentage:.1f}%)\")\n",
    "\n",
    "        print(f\"\\nAverage post score: {total_score / len(data):.1f}\")\n",
    "        print(f\"Average comments per post: {total_comments / len(data):.1f}\")\n",
    "\n",
    "        # Show some sample filenames\n",
    "        print(f\"\\nSample downloaded files:\")\n",
    "        for i, item in enumerate(data[:5]):\n",
    "            print(f\"  {item['filename']} - {item['bug_type']} - {item['post_title'][:50]}...\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Metadata file {metadata_file} not found. Run the scraper first!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing data: {e}\")\n",
    "\n",
    "def list_downloaded_images():\n",
    "    \"\"\"List all downloaded images\"\"\"\n",
    "    try:\n",
    "        images = [f for f in os.listdir('images') if f.endswith('.jpg')]\n",
    "        if images:\n",
    "            print(f\"Downloaded images ({len(images)} total):\")\n",
    "            for img in sorted(images):\n",
    "                print(f\"  {img}\")\n",
    "        else:\n",
    "            print(\"No images found in images/ directory\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Images directory not found\")\n",
    "\n",
    "# Run analysis if metadata exists\n",
    "if os.path.exists('metadata/scraping_results.json'):\n",
    "    analyze_scraped_data()\n",
    "else:\n",
    "    print(\"No metadata file found yet. Run the scraper first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db780e5",
   "metadata": {},
   "source": [
    "## Instructions for Use\n",
    "\n",
    "### Before Running the Scraper:\n",
    "\n",
    "1. **Set up Reddit API credentials:**\n",
    "   - Go to https://www.reddit.com/prefs/apps/\n",
    "   - Click \"Create App\" or \"Create Another App\"\n",
    "   - Choose \"script\" as the app type\n",
    "   - Fill in the required fields\n",
    "   - Copy your `client_id` and `client_secret`\n",
    "   - Update the credentials in the \"Reddit API Configuration\" cell above\n",
    "\n",
    "2. **Configure scraping parameters:**\n",
    "   - Adjust `POSTS_TO_SCRAPE` (recommended: start with 10-25 for testing)\n",
    "   - Choose `TIME_FILTER` ('hour', 'day', 'week', 'month', 'year', 'all')\n",
    "\n",
    "### How the Script Works:\n",
    "\n",
    "1. **Connects to Reddit** using PRAW in read-only mode\n",
    "2. **Fetches posts** from r/bugbites subreddit\n",
    "3. **Analyzes text** (title + comments) to detect bug types using keyword matching\n",
    "4. **Downloads images** from posts and saves them as `BUGTYPE_N.jpg`\n",
    "5. **Saves metadata** about each image in JSON format\n",
    "\n",
    "### Output Structure:\n",
    "```\n",
    "images/\n",
    " MOSQUITO_1.jpg\n",
    " MOSQUITO_2.jpg\n",
    " SPIDER_1.jpg\n",
    " UNKNOWN_1.jpg\n",
    " ...\n",
    "\n",
    "metadata/\n",
    " scraping_results.json\n",
    "```\n",
    "\n",
    "### Troubleshooting:\n",
    "\n",
    "- **\"Error connecting to Reddit API\"**: Check your credentials\n",
    "- **\"No images found\"**: Some posts may not contain direct image links\n",
    "- **Rate limiting**: The script includes delays to respect Reddit's API limits\n",
    "- **Imgur albums**: Currently only handles single Imgur images, not full albums\n",
    "\n",
    "### Notes:\n",
    "\n",
    "- The script respects Reddit's API rate limits with built-in delays\n",
    "- Images are classified based on text analysis of titles and comments\n",
    "- Unknown or ambiguous posts are classified as 'UNKNOWN'\n",
    "- All metadata is preserved for later analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
