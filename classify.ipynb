{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f606c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Bug classification system loaded with comment upvote weighting!\n",
      "\n",
      "ðŸ† NEW FEATURE: Comment Upvote Weighting\n",
      "   â€¢ High-scoring comments (10+ upvotes): 2.5x classification weight\n",
      "   â€¢ Medium-scoring comments (5+ upvotes): 2.0x classification weight\n",
      "   â€¢ Positive comments (2+ avg): 1.5x classification weight\n",
      "   â€¢ Slight positive (1+ avg): 1.2x classification weight\n",
      "   â€¢ Low/negative scores: No weight boost\n",
      "\n",
      "ðŸŽ¯ Benefits:\n",
      "   â€¢ Community validation improves accuracy\n",
      "   â€¢ Expert identifications get priority\n",
      "   â€¢ Uncertain comments don't mislead classification\n",
      "\n",
      "ðŸ“š Classification categories available:\n",
      "   â€¢ ant\n",
      "   â€¢ bed_bug\n",
      "   â€¢ bee\n",
      "   â€¢ chigger\n",
      "   â€¢ flea\n",
      "   â€¢ mite\n",
      "   â€¢ mosquito\n",
      "   â€¢ spider\n",
      "   â€¢ tick\n",
      "   â€¢ unknown (fallback category)\n"
     ]
    }
   ],
   "source": [
    "# Bug classification system with improved accuracy and comment upvote weighting\n",
    "\n",
    "# Define bug bite types with specific keywords (FIXED - removed overly generic terms)\n",
    "BUG_TYPES = {\n",
    "    'mosquito': [\n",
    "        'mosquito', 'mosquitos', 'mosquitoes', 'skeeter', 'skeeters',\n",
    "        'itchy', 'itching', 'itch', 'itches', 'scratching',\n",
    "        'red bumps', 'small bumps', 'tiny bumps', 'welts', 'welt',\n",
    "        'swollen', 'swelling', 'inflammation', 'inflamed'\n",
    "    ],\n",
    "    'spider': [\n",
    "        'spider', 'spiders', 'arachnid', 'eight legs', 'web',\n",
    "        'black widow', 'brown recluse', 'recluse', 'widow',\n",
    "        'fang marks', 'fangs', 'puncture', 'necrosis', 'necrotic'\n",
    "    ],\n",
    "    'bed_bug': [\n",
    "        'bed bug', 'bedbug', 'bed bugs', 'bedbugs',\n",
    "        'hotel', 'motel', 'mattress', 'bed', 'sleeping',\n",
    "        'line of bites', 'breakfast lunch dinner', 'three bites',\n",
    "        'cluster', 'clustered', 'pattern', 'row'\n",
    "    ],\n",
    "    'flea': [\n",
    "        'flea', 'fleas', 'pet', 'dog', 'cat', 'pets',\n",
    "        'ankle', 'ankles', 'lower leg', 'feet', 'foot',\n",
    "        'carpet', 'jumping', 'tiny bites'\n",
    "    ],\n",
    "    'tick': [\n",
    "        'tick', 'ticks', 'lyme', 'bullseye', 'bulls eye',\n",
    "        'hiking', 'woods', 'forest', 'outdoors', 'camping',\n",
    "        'embedded', 'attached', 'circular rash', 'rash'\n",
    "    ],\n",
    "    'ant': [\n",
    "        'ant', 'ants', 'fire ant', 'fire ants',\n",
    "        'burning', 'burn', 'stinging', 'sting',\n",
    "        'pustule', 'pustules', 'pus', 'white head'\n",
    "    ],\n",
    "    'bee': [\n",
    "        'bee', 'bees', 'wasp', 'wasps', 'hornet', 'hornets',\n",
    "        'sting', 'stinger', 'stung', 'swollen', 'allergic',\n",
    "        'yellow jacket', 'bumble bee'\n",
    "    ],\n",
    "    'chigger': [\n",
    "        'chigger', 'chiggers', 'harvest mite', 'red bug',\n",
    "        'grass', 'tall grass', 'vegetation', 'mowing',\n",
    "        'waistline', 'waist', 'belt line', 'socks'\n",
    "    ],\n",
    "    'mite': [\n",
    "        'mite', 'mites', 'dust mite', 'scabies',\n",
    "        'burrow', 'tunnels', 'between fingers', 'wrists'\n",
    "    ],\n",
    "    'unknown': [\n",
    "        'unknown', 'unidentified', 'mystery', 'unclear', 'unsure'\n",
    "        # REMOVED: 'bug', 'bite', 'what bit', 'help', 'identify' - these were too generic!\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Enhanced contextual clues for better classification\n",
    "CONTEXTUAL_CLUES = {\n",
    "    'location_hints': {\n",
    "        'bed_bug': ['bed', 'mattress', 'hotel', 'motel', 'sleeping', 'woke up'],\n",
    "        'flea': ['pet', 'dog', 'cat', 'ankle', 'lower leg', 'carpet'],\n",
    "        'mosquito': ['outside', 'evening', 'dusk', 'water', 'pond', 'lake'],\n",
    "        'tick': ['hiking', 'woods', 'forest', 'camping', 'outdoors', 'grass'],\n",
    "        'spider': ['corner', 'basement', 'garage', 'shed', 'dark'],\n",
    "        'chigger': ['grass', 'lawn', 'mowing', 'gardening']\n",
    "    },\n",
    "    'pattern_hints': {\n",
    "        'bed_bug': ['line', 'row', 'breakfast lunch dinner', 'three', 'cluster'],\n",
    "        'flea': ['multiple', 'scattered', 'random'],\n",
    "        'mosquito': ['single', 'isolated', 'few'],\n",
    "        'spider': ['two', 'pair', 'double', 'fang']\n",
    "    },\n",
    "    'symptom_hints': {\n",
    "        'mosquito': ['itchy', 'itch', 'scratching', 'red', 'swollen'],\n",
    "        'spider': ['pain', 'necrosis', 'spreading', 'severe'],\n",
    "        'bee': ['swollen', 'allergic', 'immediate', 'painful'],\n",
    "        'ant': ['burning', 'fire', 'pustule', 'pus']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Seasonal patterns for additional context\n",
    "SEASONAL_PATTERNS = {\n",
    "    'mosquito': ['summer', 'warm', 'humid', 'rain'],\n",
    "    'tick': ['spring', 'summer', 'fall', 'warm weather'],\n",
    "    'chigger': ['late summer', 'fall', 'humid']\n",
    "}\n",
    "\n",
    "def detect_bug_type_advanced(text, comments=\"\", comment_scores=None, debug=False):\n",
    "    \"\"\"\n",
    "    Advanced bug bite detection with comment upvote weighting for community validation\n",
    "\n",
    "    Args:\n",
    "        text: Post title and content to analyze\n",
    "        comments: Comments from the post\n",
    "        comment_scores: List of comment upvote scores for weighting\n",
    "        debug: Whether to print debug information\n",
    "\n",
    "    Returns:\n",
    "        Detected bug type as string\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine text and comments for analysis\n",
    "    combined_text = f\"{text} {comments}\".lower()\n",
    "\n",
    "    # Calculate comment score weighting multiplier\n",
    "    comment_weight_multiplier = 1.0\n",
    "    if comment_scores and len(comment_scores) > 0:\n",
    "        # Filter out negative scores for calculations\n",
    "        positive_scores = [score for score in comment_scores if score > 0]\n",
    "\n",
    "        if positive_scores:\n",
    "            max_score = max(comment_scores)\n",
    "            avg_score = sum(positive_scores) / len(positive_scores)\n",
    "\n",
    "            # Tiered weighting system based on community validation\n",
    "            if max_score >= 10:\n",
    "                comment_weight_multiplier = 2.5  # Very high confidence\n",
    "            elif max_score >= 5:\n",
    "                comment_weight_multiplier = 2.0  # High confidence\n",
    "            elif avg_score >= 2:\n",
    "                comment_weight_multiplier = 1.5  # Moderate confidence\n",
    "            elif avg_score >= 1:\n",
    "                comment_weight_multiplier = 1.2  # Slight confidence boost\n",
    "            # else: 1.0 (no boost for low/negative scores)\n",
    "\n",
    "            if debug:\n",
    "                print(f\"Comment scores: {comment_scores}\")\n",
    "                print(f\"Max score: {max_score}, Avg score: {avg_score:.1f}\")\n",
    "                print(f\"Weight multiplier: {comment_weight_multiplier}x\")\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    # Score each bug type\n",
    "    for bug_type, keywords in BUG_TYPES.items():\n",
    "        score = 0\n",
    "        matched_keywords = []\n",
    "\n",
    "        for keyword in keywords:\n",
    "            if keyword in combined_text:\n",
    "                # Smart weighting: generic terms get lower weight\n",
    "                if keyword in ['bug', 'bite', 'what bit', 'help', 'identify']:\n",
    "                    weight = 0.5  # Very low weight for generic terms\n",
    "                elif len(keyword) <= 3:  # Short terms\n",
    "                    weight = 1.0\n",
    "                else:\n",
    "                    weight = 2.0  # Higher weight for specific terms\n",
    "\n",
    "                score += weight\n",
    "                matched_keywords.append(keyword)\n",
    "\n",
    "        # Bonus scoring for contextual clues\n",
    "        if bug_type in CONTEXTUAL_CLUES['location_hints']:\n",
    "            for hint in CONTEXTUAL_CLUES['location_hints'][bug_type]:\n",
    "                if hint in combined_text:\n",
    "                    score += 3.0  # Location context is very valuable\n",
    "                    matched_keywords.append(f\"location:{hint}\")\n",
    "\n",
    "        if bug_type in CONTEXTUAL_CLUES['pattern_hints']:\n",
    "            for hint in CONTEXTUAL_CLUES['pattern_hints'][bug_type]:\n",
    "                if hint in combined_text:\n",
    "                    score += 2.5  # Pattern context is valuable\n",
    "                    matched_keywords.append(f\"pattern:{hint}\")\n",
    "\n",
    "        if bug_type in CONTEXTUAL_CLUES['symptom_hints']:\n",
    "            for hint in CONTEXTUAL_CLUES['symptom_hints'][bug_type]:\n",
    "                if hint in combined_text:\n",
    "                    score += 2.0  # Symptom context helps\n",
    "                    matched_keywords.append(f\"symptom:{hint}\")\n",
    "\n",
    "        # Apply comment upvote weighting to final score\n",
    "        final_score = score * comment_weight_multiplier\n",
    "        scores[bug_type] = final_score\n",
    "\n",
    "        if debug and final_score > 0:\n",
    "            print(f\"{bug_type}: {score:.1f} * {comment_weight_multiplier}x = {final_score:.1f} (keywords: {matched_keywords})\")\n",
    "\n",
    "    # Find the highest scoring type\n",
    "    if not scores or max(scores.values()) == 0:\n",
    "        if debug:\n",
    "            print(\"No matches found, defaulting to unknown\")\n",
    "        return 'unknown'\n",
    "\n",
    "    best_type = max(scores.items(), key=lambda x: x[1])\n",
    "\n",
    "    # Enhanced fallback logic for question-style titles\n",
    "    if best_type[1] < 1.0:  # Very low confidence\n",
    "        # Look for any specific clues in questions like \"What bit me?\"\n",
    "        question_indicators = ['what bit', 'what is', 'help identify', 'any idea']\n",
    "        if any(indicator in combined_text for indicator in question_indicators):\n",
    "            # Re-examine for any specific clues\n",
    "            for bug_type, keywords in BUG_TYPES.items():\n",
    "                if bug_type == 'unknown':\n",
    "                    continue\n",
    "                for keyword in keywords:\n",
    "                    if len(keyword) > 4 and keyword in combined_text:  # Only specific terms\n",
    "                        if debug:\n",
    "                            print(f\"Fallback detection: found '{keyword}' for {bug_type}\")\n",
    "                        return bug_type\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Best match: {best_type[0]} with score {best_type[1]:.1f}\")\n",
    "\n",
    "    return best_type[0]\n",
    "\n",
    "print(\"âœ… Bug classification system loaded with comment upvote weighting!\")\n",
    "print()\n",
    "print(\"ðŸ† NEW FEATURE: Comment Upvote Weighting\")\n",
    "print(\"   â€¢ High-scoring comments (10+ upvotes): 2.5x classification weight\")\n",
    "print(\"   â€¢ Medium-scoring comments (5+ upvotes): 2.0x classification weight\")\n",
    "print(\"   â€¢ Positive comments (2+ avg): 1.5x classification weight\")\n",
    "print(\"   â€¢ Slight positive (1+ avg): 1.2x classification weight\")\n",
    "print(\"   â€¢ Low/negative scores: No weight boost\")\n",
    "print()\n",
    "print(\"ðŸŽ¯ Benefits:\")\n",
    "print(\"   â€¢ Community validation improves accuracy\")\n",
    "print(\"   â€¢ Expert identifications get priority\")\n",
    "print(\"   â€¢ Uncertain comments don't mislead classification\")\n",
    "print()\n",
    "print(\"ðŸ“š Classification categories available:\")\n",
    "for bug_type in sorted(BUG_TYPES.keys()):\n",
    "    if bug_type != 'unknown':\n",
    "        print(f\"   â€¢ {bug_type}\")\n",
    "print(f\"   â€¢ unknown (fallback category)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "085a0c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Environment variables loaded from .env file\n",
      "Directories created: images/, metadata/\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import praw\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create directories for output\n",
    "os.makedirs('images', exist_ok=True)\n",
    "os.makedirs('metadata', exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Environment variables loaded from .env file\")\n",
    "print(\"Directories created: images/, metadata/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ccd7ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded credentials - Client ID: QH_iPhx9pf...\n",
      "Target subreddit: r/bugbites\n",
      "Fetch strategy: discussion_heavy\n",
      "Posts count: 25\n",
      "Reddit API connection successful!\n",
      "Read-only mode: True\n"
     ]
    }
   ],
   "source": [
    "# Reddit API Configuration\n",
    "# Credentials are loaded from .env file\n",
    "\n",
    "# Load credentials from environment variables\n",
    "REDDIT_CLIENT_ID = os.getenv('REDDIT_CLIENT_ID')\n",
    "REDDIT_CLIENT_SECRET = os.getenv('REDDIT_CLIENT_SECRET')\n",
    "REDDIT_USER_AGENT = \"PersonalApp/1.0 by reupped\"\n",
    "REDDIT_SUBREDDIT = os.getenv('REDDIT_SUBREDDIT', 'bugbites')  # Default to 'bugbites' if not set\n",
    "REDDIT_POST_FETCH_STRATEGY = os.getenv('REDDIT_POST_FETCH_STRATEGY', 'balanced')  # Default strategy\n",
    "REDDIT_POSTS_COUNT = int(os.getenv('REDDIT_POSTS_COUNT', '15'))  # Default count\n",
    "\n",
    "# Check if credentials are loaded\n",
    "if not REDDIT_CLIENT_ID or not REDDIT_CLIENT_SECRET:\n",
    "    print(\"Error: Reddit API credentials not found in .env file\")\n",
    "    print(\"Please make sure your .env file contains:\")\n",
    "    print(\"REDDIT_CLIENT_ID=your_client_id\")\n",
    "    print(\"REDDIT_CLIENT_SECRET=your_client_secret\")\n",
    "    print(\"REDDIT_SUBREDDIT=subreddit_name (optional, defaults to 'bugbites')\")\n",
    "    print(\"REDDIT_POST_FETCH_STRATEGY=strategy (optional, defaults to 'balanced')\")\n",
    "    print(\"REDDIT_POSTS_COUNT=number (optional, defaults to 15)\")\n",
    "else:\n",
    "    print(f\"Loaded credentials - Client ID: {REDDIT_CLIENT_ID[:10]}...\")\n",
    "    print(f\"Target subreddit: r/{REDDIT_SUBREDDIT}\")\n",
    "    print(f\"Fetch strategy: {REDDIT_POST_FETCH_STRATEGY}\")\n",
    "    print(f\"Posts count: {REDDIT_POSTS_COUNT}\")\n",
    "\n",
    "# Initialize Reddit instance\n",
    "try:\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=REDDIT_CLIENT_ID,\n",
    "        client_secret=REDDIT_CLIENT_SECRET,\n",
    "        user_agent=REDDIT_USER_AGENT\n",
    "    )\n",
    "\n",
    "    # Test the connection\n",
    "    print(\"Reddit API connection successful!\")\n",
    "    print(f\"Read-only mode: {reddit.read_only}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Reddit API: {e}\")\n",
    "    print(\"Please check your Reddit API credentials in the .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f96079b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File renaming utility ready!\n"
     ]
    }
   ],
   "source": [
    "# File renaming utility based on reclassification\n",
    "def rename_files_by_classification():\n",
    "    \"\"\"\n",
    "    Rename existing downloaded files based on new classifications\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the reclassified data\n",
    "        reclassified_file = 'metadata/scraping_results_reclassified.json'\n",
    "        if not os.path.exists(reclassified_file):\n",
    "            print(\"No reclassified data found. Run reclassification first!\")\n",
    "            return\n",
    "\n",
    "        with open(reclassified_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Group by new bug type and create new counters\n",
    "        new_counters = defaultdict(int)\n",
    "        rename_mapping = []\n",
    "\n",
    "        # Sort data by bug type to ensure consistent numbering\n",
    "        data_by_type = defaultdict(list)\n",
    "        for item in data:\n",
    "            data_by_type[item['bug_type']].append(item)\n",
    "\n",
    "        # Create new filenames for each type\n",
    "        for bug_type in sorted(data_by_type.keys()):\n",
    "            items = data_by_type[bug_type]\n",
    "            for item in items:\n",
    "                old_filename = item['filename']\n",
    "                new_counters[bug_type] += 1\n",
    "                new_filename = f\"images/{bug_type.upper()}_{new_counters[bug_type]}.jpg\"\n",
    "\n",
    "                if old_filename != new_filename:\n",
    "                    rename_mapping.append((old_filename, new_filename))\n",
    "                    item['filename'] = new_filename  # Update metadata\n",
    "\n",
    "        # Perform the actual renaming\n",
    "        renamed_count = 0\n",
    "        for old_path, new_path in rename_mapping:\n",
    "            if os.path.exists(old_path):\n",
    "                # Ensure no conflict with existing files\n",
    "                if os.path.exists(new_path):\n",
    "                    # Create a temporary name to avoid conflicts\n",
    "                    temp_path = f\"{new_path}.temp\"\n",
    "                    os.rename(old_path, temp_path)\n",
    "                    old_path = temp_path\n",
    "\n",
    "                os.rename(old_path, new_path)\n",
    "                renamed_count += 1\n",
    "                print(f\"Renamed: {old_path} -> {new_path}\")\n",
    "\n",
    "        # Save updated metadata\n",
    "        with open(reclassified_file, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "        print(f\"\\nFile renaming complete!\")\n",
    "        print(f\"Renamed {renamed_count} files\")\n",
    "        print(f\"Updated metadata saved to {reclassified_file}\")\n",
    "\n",
    "        # Show current file structure\n",
    "        print(f\"\\nCurrent images directory:\")\n",
    "        try:\n",
    "            images = sorted([f for f in os.listdir('images') if f.endswith('.jpg')])\n",
    "            type_counts = defaultdict(int)\n",
    "            for img in images:\n",
    "                bug_type = img.split('_')[0].lower()\n",
    "                type_counts[bug_type] += 1\n",
    "\n",
    "            for bug_type, count in sorted(type_counts.items()):\n",
    "                print(f\"  {bug_type.upper()}: {count} files\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(\"  No images directory found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during file renaming: {e}\")\n",
    "\n",
    "print(\"File renaming utility ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe1eeceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image processing functions with run management ready!\n"
     ]
    }
   ],
   "source": [
    "# Image download and processing functions with timestamped runs\n",
    "def create_run_directory():\n",
    "    \"\"\"Create a timestamped directory for this scraping run\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = f\"images/run{timestamp}\"\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Created run directory: {run_dir}\")\n",
    "    return run_dir, timestamp\n",
    "\n",
    "def is_image_url(url):\n",
    "    \"\"\"Check if URL points to an image\"\"\"\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp']\n",
    "    parsed_url = urlparse(url.lower())\n",
    "    return any(parsed_url.path.endswith(ext) for ext in image_extensions)\n",
    "\n",
    "def get_reddit_image_urls(submission):\n",
    "    \"\"\"Extract image URLs from a Reddit submission\"\"\"\n",
    "    urls = []\n",
    "\n",
    "    # Direct image link\n",
    "    if hasattr(submission, 'url') and is_image_url(submission.url):\n",
    "        urls.append(submission.url)\n",
    "\n",
    "    # Reddit gallery\n",
    "    if hasattr(submission, 'is_gallery') and submission.is_gallery:\n",
    "        try:\n",
    "            for item in submission.gallery_data['items']:\n",
    "                media_id = item['media_id']\n",
    "                if media_id in submission.media_metadata:\n",
    "                    media_info = submission.media_metadata[media_id]\n",
    "                    if 's' in media_info and 'u' in media_info['s']:\n",
    "                        # Convert preview URL to full resolution\n",
    "                        url = media_info['s']['u'].replace('preview.redd.it', 'i.redd.it')\n",
    "                        url = url.split('?')[0]  # Remove query parameters\n",
    "                        urls.append(url)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error processing gallery: {e}\")\n",
    "\n",
    "    # Check if it's an Imgur link\n",
    "    if 'imgur.com' in submission.url:\n",
    "        # Convert imgur links to direct image links\n",
    "        if '/a/' in submission.url or '/gallery/' in submission.url:\n",
    "            # Album/gallery - would need imgur API for full access\n",
    "            logger.info(f\"Imgur album detected: {submission.url}\")\n",
    "        else:\n",
    "            # Single image\n",
    "            imgur_id = submission.url.split('/')[-1].split('.')[0]\n",
    "            direct_url = f\"https://i.imgur.com/{imgur_id}.jpg\"\n",
    "            urls.append(direct_url)\n",
    "\n",
    "    return urls\n",
    "\n",
    "def download_image(url, filename):\n",
    "    \"\"\"Download an image from URL and save it\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Check if response is actually an image\n",
    "        content_type = response.headers.get('content-type', '')\n",
    "        if not content_type.startswith('image/'):\n",
    "            logger.warning(f\"URL doesn't return an image: {url}\")\n",
    "            return False\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        logger.info(f\"Downloaded: {filename}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error downloading {url}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Global variables for run management\n",
    "current_run_dir = None\n",
    "current_timestamp = None\n",
    "bug_counters = defaultdict(int)\n",
    "\n",
    "def initialize_new_run():\n",
    "    \"\"\"Initialize a new scraping run with timestamped directory\"\"\"\n",
    "    global current_run_dir, current_timestamp, bug_counters\n",
    "\n",
    "    current_run_dir, current_timestamp = create_run_directory()\n",
    "    bug_counters = defaultdict(int)  # Reset counters for new run\n",
    "\n",
    "    return current_run_dir, current_timestamp\n",
    "\n",
    "def get_next_filename(bug_type):\n",
    "    \"\"\"Get the next filename for a bug type in the current run\"\"\"\n",
    "    global current_run_dir, bug_counters\n",
    "\n",
    "    if current_run_dir is None:\n",
    "        # Initialize if not already done\n",
    "        initialize_new_run()\n",
    "\n",
    "    bug_counters[bug_type] += 1\n",
    "    return f\"{current_run_dir}/{bug_type.upper()}_{bug_counters[bug_type]}.jpg\"\n",
    "\n",
    "def get_run_summary():\n",
    "    \"\"\"Get summary of the current run\"\"\"\n",
    "    global current_run_dir, current_timestamp, bug_counters\n",
    "\n",
    "    if current_run_dir is None:\n",
    "        return \"No active run\"\n",
    "\n",
    "    summary = {\n",
    "        'run_directory': current_run_dir,\n",
    "        'timestamp': current_timestamp,\n",
    "        'bug_type_counts': dict(bug_counters),\n",
    "        'total_images': sum(bug_counters.values())\n",
    "    }\n",
    "\n",
    "    return summary\n",
    "\n",
    "print(\"Image processing functions with run management ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc82d657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced main scraping function with engagement-based sorting ready!\n"
     ]
    }
   ],
   "source": [
    "# Main scraping function with run management and improved post selection\n",
    "def scrape_bugbites_subreddit(limit=50, time_filter='week', sort_method='top'):\n",
    "    \"\"\"\n",
    "    Scrape a configurable subreddit for images and classify them\n",
    "    Subreddit is set via REDDIT_SUBREDDIT env variable (defaults to 'bugbites')\n",
    "    Each run gets its own timestamped directory\n",
    "\n",
    "    Args:\n",
    "        limit: Number of posts to scrape\n",
    "        time_filter: Time filter for posts ('hour', 'day', 'week', 'month', 'year', 'all')\n",
    "        sort_method: How to sort posts ('top', 'hot', 'new', 'controversial', 'rising')\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize new run\n",
    "    run_dir, timestamp = initialize_new_run()\n",
    "    scraped_data = []\n",
    "\n",
    "    print(f\"Starting new scraping run: {timestamp}\")\n",
    "    print(f\"Target subreddit: r/{REDDIT_SUBREDDIT}\")\n",
    "    print(f\"Images will be saved to: {run_dir}\")\n",
    "    print(f\"Sort method: {sort_method}, Time filter: {time_filter}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        subreddit = reddit.subreddit(REDDIT_SUBREDDIT)\n",
    "\n",
    "        # Get posts from the subreddit using different sorting methods\n",
    "        if sort_method == 'top':\n",
    "            posts = subreddit.top(time_filter=time_filter, limit=limit)\n",
    "            print(f\"Fetching top {limit} posts from the past {time_filter}\")\n",
    "        elif sort_method == 'hot':\n",
    "            posts = subreddit.hot(limit=limit)\n",
    "            print(f\"Fetching {limit} hot posts\")\n",
    "        elif sort_method == 'new':\n",
    "            posts = subreddit.new(limit=limit)\n",
    "            print(f\"Fetching {limit} newest posts\")\n",
    "        elif sort_method == 'controversial':\n",
    "            posts = subreddit.controversial(time_filter=time_filter, limit=limit)\n",
    "            print(f\"Fetching {limit} controversial posts from the past {time_filter}\")\n",
    "        elif sort_method == 'rising':\n",
    "            posts = subreddit.rising(limit=limit)\n",
    "            print(f\"Fetching {limit} rising posts\")\n",
    "        else:\n",
    "            # Default to top if invalid method\n",
    "            posts = subreddit.top(time_filter=time_filter, limit=limit)\n",
    "            print(f\"Unknown sort method '{sort_method}', defaulting to top posts\")\n",
    "\n",
    "        # Collect posts first to analyze engagement\n",
    "        posts_list = list(posts)\n",
    "\n",
    "        # Sort by number of comments (descending) to prioritize posts with more discussion\n",
    "        posts_with_engagement = []\n",
    "        for submission in posts_list:\n",
    "            posts_with_engagement.append({\n",
    "                'submission': submission,\n",
    "                'num_comments': submission.num_comments,\n",
    "                'score': submission.score,\n",
    "                'engagement_score': submission.num_comments * 2 + submission.score  # Weight comments more heavily\n",
    "            })\n",
    "\n",
    "        # Sort by engagement score (comments weighted more heavily than upvotes)\n",
    "        posts_with_engagement.sort(key=lambda x: x['engagement_score'], reverse=True)\n",
    "\n",
    "        print(f\"\\nPost engagement analysis:\")\n",
    "        print(\"Top 5 posts by engagement (comments weighted 2x):\")\n",
    "        for i, post_data in enumerate(posts_with_engagement[:5]):\n",
    "            submission = post_data['submission']\n",
    "            print(f\"  {i+1}. Comments: {post_data['num_comments']}, Score: {post_data['score']}, \"\n",
    "                  f\"Engagement: {post_data['engagement_score']}\")\n",
    "            print(f\"     Title: {submission.title[:70]}...\")\n",
    "\n",
    "        print(f\"\\nProcessing posts in order of engagement:\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        for post_count, post_data in enumerate(posts_with_engagement, 1):\n",
    "            submission = post_data['submission']\n",
    "\n",
    "            print(f\"\\n--- Processing post {post_count}/{len(posts_with_engagement)} ---\")\n",
    "            print(f\"Title: {submission.title[:80]}...\")\n",
    "            print(f\"Comments: {submission.num_comments}, Score: {submission.score}\")\n",
    "\n",
    "            # Analyze title and selftext for bug type\n",
    "            combined_text = f\"{submission.title} {submission.selftext}\"\n",
    "\n",
    "            # Load more comments since we're prioritizing posts with comments\n",
    "            submission.comments.replace_more(limit=2)  # Load more comment threads\n",
    "            comments_text = \"\"\n",
    "            comment_count = 0\n",
    "            comment_scores = []  # Track comment upvotes for weighting\n",
    "            total_comment_score = 0\n",
    "\n",
    "            # Get more comments for better classification with upvote tracking\n",
    "            for comment in submission.comments.list()[:20]:  # Increased from 10 to 20\n",
    "                if hasattr(comment, 'body') and len(comment.body) > 10:  # Skip very short comments\n",
    "                    comment_score = getattr(comment, 'score', 0)  # Get comment upvotes\n",
    "                    comments_text += f\" {comment.body}\"\n",
    "                    comment_scores.append(comment_score)\n",
    "                    total_comment_score += max(comment_score, 0)  # Don't count negative scores\n",
    "                    comment_count += 1\n",
    "\n",
    "            print(f\"Loaded {comment_count} comments for analysis\")\n",
    "            if comment_scores:\n",
    "                avg_comment_score = total_comment_score / comment_count\n",
    "                max_comment_score = max(comment_scores)\n",
    "                print(f\"Comment upvotes: avg={avg_comment_score:.1f}, max={max_comment_score}, total={total_comment_score}\")\n",
    "\n",
    "            # Use the advanced detection system with comments and upvote weighting\n",
    "            bug_type = detect_bug_type_advanced(\n",
    "                combined_text,\n",
    "                comments_text,\n",
    "                comment_scores=comment_scores,\n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            print(f\"Detected bug type: {bug_type}\")\n",
    "\n",
    "            # Get image URLs\n",
    "            image_urls = get_reddit_image_urls(submission)\n",
    "\n",
    "            if image_urls:\n",
    "                print(f\"Found {len(image_urls)} image(s)\")\n",
    "\n",
    "                for img_url in image_urls:\n",
    "                    filename = get_next_filename(bug_type)\n",
    "\n",
    "                    if download_image(img_url, filename):\n",
    "                        # Store metadata with run information, engagement data, and comment scores\n",
    "                        metadata = {\n",
    "                            'run_timestamp': timestamp,\n",
    "                            'run_directory': run_dir,\n",
    "                            'filename': filename,\n",
    "                            'bug_type': bug_type,\n",
    "                            'post_title': submission.title,\n",
    "                            'post_url': f\"https://reddit.com{submission.permalink}\",\n",
    "                            'image_url': img_url,\n",
    "                            'post_score': submission.score,\n",
    "                            'num_comments': submission.num_comments,\n",
    "                            'engagement_score': post_data['engagement_score'],\n",
    "                            'comments_analyzed': comment_count,\n",
    "                            'comment_scores': comment_scores,  # NEW: Store individual comment scores\n",
    "                            'total_comment_score': total_comment_score,  # NEW: Sum of positive comment scores\n",
    "                            'avg_comment_score': total_comment_score / comment_count if comment_count > 0 else 0,  # NEW: Average comment score\n",
    "                            'max_comment_score': max(comment_scores) if comment_scores else 0,  # NEW: Highest comment score\n",
    "                            'sort_method': sort_method,\n",
    "                            'time_filter': time_filter,\n",
    "                            'created_utc': submission.created_utc,\n",
    "                            'author': str(submission.author) if submission.author else '[deleted]',\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        }\n",
    "                        scraped_data.append(metadata)\n",
    "\n",
    "                        print(f\"Saved as: {filename}\")\n",
    "            else:\n",
    "                print(\"No images found in this post\")\n",
    "\n",
    "            # Be respectful to Reddit's API - slightly longer delay for comment loading\n",
    "            time.sleep(1.5)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error scraping subreddit: {e}\")\n",
    "\n",
    "    return scraped_data, timestamp\n",
    "\n",
    "def save_metadata_with_run(scraped_data, timestamp):\n",
    "    \"\"\"Save scraping metadata with run-specific information\"\"\"\n",
    "\n",
    "    # Save run-specific metadata\n",
    "    run_metadata_file = f'metadata/scraping_results_run{timestamp}.json'\n",
    "    with open(run_metadata_file, 'w') as f:\n",
    "        json.dump(scraped_data, f, indent=2)\n",
    "\n",
    "    # Also append to master metadata file\n",
    "    master_metadata_file = 'metadata/all_scraping_results.json'\n",
    "\n",
    "    # Load existing master data if it exists\n",
    "    all_data = []\n",
    "    if os.path.exists(master_metadata_file):\n",
    "        try:\n",
    "            with open(master_metadata_file, 'r') as f:\n",
    "                all_data = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            all_data = []\n",
    "\n",
    "    # Add new data\n",
    "    all_data.extend(scraped_data)\n",
    "\n",
    "    # Save updated master file\n",
    "    with open(master_metadata_file, 'w') as f:\n",
    "        json.dump(all_data, f, indent=2)\n",
    "\n",
    "    print(f\"Run metadata saved to: {run_metadata_file}\")\n",
    "    print(f\"Master metadata updated: {master_metadata_file}\")\n",
    "\n",
    "    # Save run summary with engagement statistics\n",
    "    run_summary = get_run_summary()\n",
    "    run_summary['metadata_file'] = run_metadata_file\n",
    "    run_summary['total_posts_processed'] = len(scraped_data)\n",
    "\n",
    "    # Add engagement statistics\n",
    "    if scraped_data:\n",
    "        total_comments = sum(item.get('comments_analyzed', 0) for item in scraped_data)\n",
    "        avg_engagement = sum(item.get('engagement_score', 0) for item in scraped_data) / len(scraped_data)\n",
    "        run_summary['total_comments_analyzed'] = total_comments\n",
    "        run_summary['average_engagement_score'] = avg_engagement\n",
    "        run_summary['sort_method'] = scraped_data[0].get('sort_method', 'unknown')\n",
    "        run_summary['time_filter'] = scraped_data[0].get('time_filter', 'unknown')\n",
    "\n",
    "    summary_file = f'metadata/run_summary_{timestamp}.json'\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(run_summary, f, indent=2)\n",
    "\n",
    "    print(f\"Run summary saved to: {summary_file}\")\n",
    "\n",
    "    return run_metadata_file, master_metadata_file\n",
    "\n",
    "def list_all_runs():\n",
    "    \"\"\"List all previous scraping runs\"\"\"\n",
    "    try:\n",
    "        runs = []\n",
    "\n",
    "        # Look for run directories\n",
    "        if os.path.exists('images'):\n",
    "            for item in os.listdir('images'):\n",
    "                if item.startswith('run') and os.path.isdir(f'images/{item}'):\n",
    "                    timestamp = item[3:]  # Remove 'run' prefix\n",
    "\n",
    "                    # Count files in directory\n",
    "                    run_dir = f'images/{item}'\n",
    "                    image_count = len([f for f in os.listdir(run_dir) if f.endswith('.jpg')])\n",
    "\n",
    "                    # Try to load summary if available\n",
    "                    summary_file = f'metadata/run_summary_{timestamp}.json'\n",
    "                    bug_counts = {}\n",
    "                    sort_method = 'unknown'\n",
    "                    engagement_info = {}\n",
    "\n",
    "                    if os.path.exists(summary_file):\n",
    "                        with open(summary_file, 'r') as f:\n",
    "                            summary = json.load(f)\n",
    "                            bug_counts = summary.get('bug_type_counts', {})\n",
    "                            sort_method = summary.get('sort_method', 'unknown')\n",
    "                            engagement_info = {\n",
    "                                'total_comments': summary.get('total_comments_analyzed', 0),\n",
    "                                'avg_engagement': summary.get('average_engagement_score', 0)\n",
    "                            }\n",
    "\n",
    "                    runs.append({\n",
    "                        'timestamp': timestamp,\n",
    "                        'directory': run_dir,\n",
    "                        'image_count': image_count,\n",
    "                        'bug_counts': bug_counts,\n",
    "                        'sort_method': sort_method,\n",
    "                        'engagement_info': engagement_info\n",
    "                    })\n",
    "\n",
    "        # Sort by timestamp\n",
    "        runs.sort(key=lambda x: x['timestamp'], reverse=True)\n",
    "\n",
    "        if runs:\n",
    "            print(f\"Found {len(runs)} previous runs:\")\n",
    "            print(\"-\" * 100)\n",
    "            for run in runs:\n",
    "                print(f\"Run {run['timestamp']}: {run['image_count']} images, \"\n",
    "                      f\"Sort: {run['sort_method']}, \"\n",
    "                      f\"Comments: {run['engagement_info'].get('total_comments', 0)}\")\n",
    "                print(f\"  Directory: {run['directory']}\")\n",
    "                if run['bug_counts']:\n",
    "                    bug_summary = \", \".join([f\"{bt.upper()}: {cnt}\" for bt, cnt in run['bug_counts'].items()])\n",
    "                    print(f\"  Types: {bug_summary}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"No previous runs found\")\n",
    "\n",
    "        return runs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing runs: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"Enhanced main scraping function with engagement-based sorting ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ae67062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new Reddit scraping run with enhanced post selection...\n",
      "Configuration:\n",
      "  Target subreddit: r/bugbites\n",
      "  Fetch strategy: discussion_heavy\n",
      "  Posts to scrape: 25\n",
      "  Time filter: all\n",
      "  Sort method: controversial\n",
      "  Strategy: Prioritizing posts with more comments for better classification\n",
      "======================================================================\n",
      "\n",
      "Previous runs:\n",
      "Found 6 previous runs:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Run 20250806_203307: 36 images, Sort: top, Comments: 271\n",
      "  Directory: images/run20250806_203307\n",
      "  Types: MOSQUITO: 14, BED_BUG: 13, ANT: 1, TICK: 3, FLEA: 5\n",
      "\n",
      "Run 20250806_201819: 36 images, Sort: top, Comments: 271\n",
      "  Directory: images/run20250806_201819\n",
      "  Types: TICK: 5, BED_BUG: 16, DERMATITIS: 1, SCABIES: 1, SPIDER: 2, FLEA: 2, MITE: 4, BEE: 5\n",
      "\n",
      "Run 20250806_201209: 36 images, Sort: top, Comments: 271\n",
      "  Directory: images/run20250806_201209\n",
      "  Types: TICK: 1, UNKNOWN: 24, BED_BUG: 10, SCABIES: 1\n",
      "\n",
      "Run 20250806_200730: 36 images, Sort: top, Comments: 271\n",
      "  Directory: images/run20250806_200730\n",
      "  Types: TICK: 5, BED_BUG: 16, ANT: 1, MITE: 5, SPIDER: 2, FLEA: 2, BEE: 5\n",
      "\n",
      "Run 20250806_200212: 19 images, Sort: unknown, Comments: 0\n",
      "  Directory: images/run20250806_200212\n",
      "  Types: BED_BUG: 3, MOSQUITO: 8, UNKNOWN: 8\n",
      "\n",
      "Run 0: 19 images, Sort: unknown, Comments: 0\n",
      "  Directory: images/run0\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STARTING NEW RUN WITH ENHANCED POST SELECTION\n",
      "======================================================================\n",
      "Created run directory: images/run20250806_203706\n",
      "Starting new scraping run: 20250806_203706\n",
      "Target subreddit: r/bugbites\n",
      "Images will be saved to: images/run20250806_203706\n",
      "Sort method: controversial, Time filter: all\n",
      "============================================================\n",
      "Fetching 25 controversial posts from the past all\n",
      "\n",
      "Post engagement analysis:\n",
      "Top 5 posts by engagement (comments weighted 2x):\n",
      "  1. Comments: 29, Score: 0, Engagement: 58\n",
      "     Title: Please help! Is this bed bugs...\n",
      "  2. Comments: 20, Score: 0, Engagement: 40\n",
      "     Title: Are these bed bug bites?...\n",
      "  3. Comments: 13, Score: 0, Engagement: 26\n",
      "     Title: What bit my toddler while sleeping? ...\n",
      "  4. Comments: 8, Score: 0, Engagement: 16\n",
      "     Title: Anyone know what this could be?...\n",
      "  5. Comments: 4, Score: 0, Engagement: 8\n",
      "     Title: Can anyone identify this bite....\n",
      "\n",
      "Processing posts in order of engagement:\n",
      "============================================================\n",
      "\n",
      "--- Processing post 1/25 ---\n",
      "Title: Please help! Is this bed bugs...\n",
      "Comments: 29, Score: 0\n",
      "\n",
      "Post engagement analysis:\n",
      "Top 5 posts by engagement (comments weighted 2x):\n",
      "  1. Comments: 29, Score: 0, Engagement: 58\n",
      "     Title: Please help! Is this bed bugs...\n",
      "  2. Comments: 20, Score: 0, Engagement: 40\n",
      "     Title: Are these bed bug bites?...\n",
      "  3. Comments: 13, Score: 0, Engagement: 26\n",
      "     Title: What bit my toddler while sleeping? ...\n",
      "  4. Comments: 8, Score: 0, Engagement: 16\n",
      "     Title: Anyone know what this could be?...\n",
      "  5. Comments: 4, Score: 0, Engagement: 8\n",
      "     Title: Can anyone identify this bite....\n",
      "\n",
      "Processing posts in order of engagement:\n",
      "============================================================\n",
      "\n",
      "--- Processing post 1/25 ---\n",
      "Title: Please help! Is this bed bugs...\n",
      "Comments: 29, Score: 0\n",
      "Loaded 16 comments for analysis\n",
      "Comment upvotes: avg=2.2, max=11, total=35\n",
      "Detected bug type: bed_bug\n",
      "Found 1 image(s)\n",
      "Loaded 16 comments for analysis\n",
      "Comment upvotes: avg=2.2, max=11, total=35\n",
      "Detected bug type: bed_bug\n",
      "Found 1 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/BED_BUG_1.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/BED_BUG_1.jpg\n",
      "\n",
      "--- Processing post 2/25 ---\n",
      "Title: Are these bed bug bites?...\n",
      "Comments: 20, Score: 0\n",
      "\n",
      "--- Processing post 2/25 ---\n",
      "Title: Are these bed bug bites?...\n",
      "Comments: 20, Score: 0\n",
      "Loaded 17 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=17\n",
      "Detected bug type: bed_bug\n",
      "Found 7 image(s)\n",
      "Loaded 17 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=17\n",
      "Detected bug type: bed_bug\n",
      "Found 7 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/BED_BUG_2.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/BED_BUG_2.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/BED_BUG_3.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/BED_BUG_4.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/BED_BUG_4.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/BED_BUG_3.jpg\n",
      "Saved as: images/run20250806_203706/BED_BUG_4.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/BED_BUG_5.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/BED_BUG_6.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/BED_BUG_6.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/BED_BUG_5.jpg\n",
      "Saved as: images/run20250806_203706/BED_BUG_6.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/BED_BUG_7.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/BED_BUG_8.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/BED_BUG_8.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/BED_BUG_7.jpg\n",
      "Saved as: images/run20250806_203706/BED_BUG_8.jpg\n",
      "\n",
      "--- Processing post 3/25 ---\n",
      "Title: What bit my toddler while sleeping? ...\n",
      "Comments: 13, Score: 0\n",
      "\n",
      "--- Processing post 3/25 ---\n",
      "Title: What bit my toddler while sleeping? ...\n",
      "Comments: 13, Score: 0\n",
      "Loaded 12 comments for analysis\n",
      "Comment upvotes: avg=1.1, max=2, total=13\n",
      "Detected bug type: mosquito\n",
      "Found 3 image(s)\n",
      "Loaded 12 comments for analysis\n",
      "Comment upvotes: avg=1.1, max=2, total=13\n",
      "Detected bug type: mosquito\n",
      "Found 3 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_1.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/MOSQUITO_1.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_2.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/MOSQUITO_2.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_3.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/MOSQUITO_3.jpg\n",
      "\n",
      "--- Processing post 4/25 ---\n",
      "Title: Anyone know what this could be?...\n",
      "Comments: 8, Score: 0\n",
      "Loaded 8 comments for analysis\n",
      "Comment upvotes: avg=1.6, max=4, total=13\n",
      "Detected bug type: bed_bug\n",
      "Found 1 image(s)\n",
      "\n",
      "--- Processing post 4/25 ---\n",
      "Title: Anyone know what this could be?...\n",
      "Comments: 8, Score: 0\n",
      "Loaded 8 comments for analysis\n",
      "Comment upvotes: avg=1.6, max=4, total=13\n",
      "Detected bug type: bed_bug\n",
      "Found 1 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/BED_BUG_9.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/BED_BUG_9.jpg\n",
      "\n",
      "--- Processing post 5/25 ---\n",
      "Title: Can anyone identify this bite....\n",
      "Comments: 4, Score: 0\n",
      "\n",
      "--- Processing post 5/25 ---\n",
      "Title: Can anyone identify this bite....\n",
      "Comments: 4, Score: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/ANT_1.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 comments for analysis\n",
      "Comment upvotes: avg=1.5, max=2, total=3\n",
      "Detected bug type: ant\n",
      "Found 3 image(s)\n",
      "Saved as: images/run20250806_203706/ANT_1.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/ANT_2.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/ANT_3.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/ANT_3.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/ANT_2.jpg\n",
      "Saved as: images/run20250806_203706/ANT_3.jpg\n",
      "\n",
      "--- Processing post 6/25 ---\n",
      "Title: My husband has had this on his leg for 3 days now....\n",
      "Comments: 4, Score: 0\n",
      "\n",
      "--- Processing post 6/25 ---\n",
      "Title: My husband has had this on his leg for 3 days now....\n",
      "Comments: 4, Score: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/SPIDER_1.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 comments for analysis\n",
      "Comment upvotes: avg=1.8, max=3, total=7\n",
      "Detected bug type: spider\n",
      "Found 2 image(s)\n",
      "Saved as: images/run20250806_203706/SPIDER_1.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/SPIDER_2.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/SPIDER_2.jpg\n",
      "\n",
      "--- Processing post 7/25 ---\n",
      "Title: What type of bite is this? New bites weekly...\n",
      "Comments: 3, Score: 0\n",
      "Loaded 3 comments for analysis\n",
      "Comment upvotes: avg=1.3, max=2, total=4\n",
      "Detected bug type: flea\n",
      "Found 2 image(s)\n",
      "\n",
      "--- Processing post 7/25 ---\n",
      "Title: What type of bite is this? New bites weekly...\n",
      "Comments: 3, Score: 0\n",
      "Loaded 3 comments for analysis\n",
      "Comment upvotes: avg=1.3, max=2, total=4\n",
      "Detected bug type: flea\n",
      "Found 2 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/FLEA_1.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/FLEA_2.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/FLEA_2.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/FLEA_1.jpg\n",
      "Saved as: images/run20250806_203706/FLEA_2.jpg\n",
      "\n",
      "--- Processing post 8/25 ---\n",
      "Title: freaking out. what bug bite is this weird circle on my hand...\n",
      "Comments: 3, Score: 0\n",
      "Loaded 3 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=3\n",
      "Detected bug type: flea\n",
      "Found 1 image(s)\n",
      "\n",
      "--- Processing post 8/25 ---\n",
      "Title: freaking out. what bug bite is this weird circle on my hand...\n",
      "Comments: 3, Score: 0\n",
      "Loaded 3 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=3\n",
      "Detected bug type: flea\n",
      "Found 1 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/FLEA_3.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/FLEA_3.jpg\n",
      "\n",
      "--- Processing post 9/25 ---\n",
      "Title: Help...\n",
      "Comments: 3, Score: 0\n",
      "Loaded 3 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=3\n",
      "Detected bug type: bed_bug\n",
      "Found 1 image(s)\n",
      "\n",
      "--- Processing post 9/25 ---\n",
      "Title: Help...\n",
      "Comments: 3, Score: 0\n",
      "Loaded 3 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=3\n",
      "Detected bug type: bed_bug\n",
      "Found 1 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/BED_BUG_10.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/BED_BUG_10.jpg\n",
      "\n",
      "--- Processing post 10/25 ---\n",
      "Title: Pls help is this tick in there...\n",
      "Comments: 3, Score: 0\n",
      "Loaded 3 comments for analysis\n",
      "Comment upvotes: avg=2.0, max=3, total=6\n",
      "Detected bug type: flea\n",
      "Found 2 image(s)\n",
      "\n",
      "--- Processing post 10/25 ---\n",
      "Title: Pls help is this tick in there...\n",
      "Comments: 3, Score: 0\n",
      "Loaded 3 comments for analysis\n",
      "Comment upvotes: avg=2.0, max=3, total=6\n",
      "Detected bug type: flea\n",
      "Found 2 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/FLEA_4.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/FLEA_5.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/FLEA_5.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/FLEA_4.jpg\n",
      "Saved as: images/run20250806_203706/FLEA_5.jpg\n",
      "\n",
      "--- Processing post 11/25 ---\n",
      "Title: The heck did this??...\n",
      "Comments: 2, Score: 0\n",
      "Loaded 2 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=2\n",
      "Detected bug type: mosquito\n",
      "Found 2 image(s)\n",
      "\n",
      "--- Processing post 11/25 ---\n",
      "Title: The heck did this??...\n",
      "Comments: 2, Score: 0\n",
      "Loaded 2 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=2\n",
      "Detected bug type: mosquito\n",
      "Found 2 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_4.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_5.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_5.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/MOSQUITO_4.jpg\n",
      "Saved as: images/run20250806_203706/MOSQUITO_5.jpg\n",
      "\n",
      "--- Processing post 12/25 ---\n",
      "Title: Whaat is this?...\n",
      "Comments: 2, Score: 0\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: flea\n",
      "Found 1 image(s)\n",
      "\n",
      "--- Processing post 12/25 ---\n",
      "Title: Whaat is this?...\n",
      "Comments: 2, Score: 0\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: flea\n",
      "Found 1 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/FLEA_6.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/FLEA_6.jpg\n",
      "\n",
      "--- Processing post 13/25 ---\n",
      "Title: Won't stop itching...\n",
      "Comments: 2, Score: 0\n",
      "Loaded 2 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=2\n",
      "Detected bug type: mosquito\n",
      "Found 3 image(s)\n",
      "\n",
      "--- Processing post 13/25 ---\n",
      "Title: Won't stop itching...\n",
      "Comments: 2, Score: 0\n",
      "Loaded 2 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=2\n",
      "Detected bug type: mosquito\n",
      "Found 3 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_6.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/MOSQUITO_6.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_7.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_8.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_8.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/MOSQUITO_7.jpg\n",
      "Saved as: images/run20250806_203706/MOSQUITO_8.jpg\n",
      "\n",
      "--- Processing post 14/25 ---\n",
      "Title: Bugbites...\n",
      "Comments: 1, Score: 0\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: flea\n",
      "Found 1 image(s)\n",
      "\n",
      "--- Processing post 14/25 ---\n",
      "Title: Bugbites...\n",
      "Comments: 1, Score: 0\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: flea\n",
      "Found 1 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/FLEA_7.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/FLEA_7.jpg\n",
      "\n",
      "--- Processing post 15/25 ---\n",
      "Title: Juneau AK - What is this...\n",
      "Comments: 1, Score: 0\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: mosquito\n",
      "Found 1 image(s)\n",
      "\n",
      "--- Processing post 15/25 ---\n",
      "Title: Juneau AK - What is this...\n",
      "Comments: 1, Score: 0\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: mosquito\n",
      "Found 1 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_9.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/MOSQUITO_9.jpg\n",
      "\n",
      "--- Processing post 16/25 ---\n",
      "Title: Are these bed bugs??...\n",
      "Comments: 1, Score: 0\n",
      "\n",
      "--- Processing post 16/25 ---\n",
      "Title: Are these bed bugs??...\n",
      "Comments: 1, Score: 0\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: bed_bug\n",
      "No images found in this post\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: bed_bug\n",
      "No images found in this post\n",
      "\n",
      "--- Processing post 17/25 ---\n",
      "Title: I have these bites on my lower legâ€”what bit me?...\n",
      "Comments: 1, Score: 0\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: flea\n",
      "Found 2 image(s)\n",
      "\n",
      "--- Processing post 17/25 ---\n",
      "Title: I have these bites on my lower legâ€”what bit me?...\n",
      "Comments: 1, Score: 0\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: flea\n",
      "Found 2 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/FLEA_8.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/FLEA_8.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/FLEA_9.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/FLEA_9.jpg\n",
      "\n",
      "--- Processing post 18/25 ---\n",
      "Title: Bed bugs or fleas or ?...\n",
      "Comments: 1, Score: 0\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: bed_bug\n",
      "Found 1 image(s)\n",
      "\n",
      "--- Processing post 18/25 ---\n",
      "Title: Bed bugs or fleas or ?...\n",
      "Comments: 1, Score: 0\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: bed_bug\n",
      "Found 1 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/BED_BUG_11.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/BED_BUG_11.jpg\n",
      "\n",
      "--- Processing post 19/25 ---\n",
      "Title: Is this just a mozzie bite ?...\n",
      "Comments: 1, Score: 0\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: mosquito\n",
      "Found 1 image(s)\n",
      "\n",
      "--- Processing post 19/25 ---\n",
      "Title: Is this just a mozzie bite ?...\n",
      "Comments: 1, Score: 0\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: mosquito\n",
      "Found 1 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_10.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/MOSQUITO_10.jpg\n",
      "\n",
      "--- Processing post 20/25 ---\n",
      "Title: Large bite -kinda nsfw...\n",
      "Comments: 1, Score: 0\n",
      "\n",
      "--- Processing post 20/25 ---\n",
      "Title: Large bite -kinda nsfw...\n",
      "Comments: 1, Score: 0\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: mosquito\n",
      "Found 2 image(s)\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: mosquito\n",
      "Found 2 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_11.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/MOSQUITO_11.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_12.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/MOSQUITO_12.jpg\n",
      "\n",
      "--- Processing post 21/25 ---\n",
      "Title: Bugs...\n",
      "Comments: 1, Score: 0\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: flea\n",
      "No images found in this post\n",
      "\n",
      "--- Processing post 21/25 ---\n",
      "Title: Bugs...\n",
      "Comments: 1, Score: 0\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: flea\n",
      "No images found in this post\n",
      "\n",
      "--- Processing post 22/25 ---\n",
      "Title: What bug are these bites from!...\n",
      "Comments: 1, Score: 0\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: bed_bug\n",
      "Found 2 image(s)\n",
      "\n",
      "--- Processing post 22/25 ---\n",
      "Title: What bug are these bites from!...\n",
      "Comments: 1, Score: 0\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: bed_bug\n",
      "Found 2 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/BED_BUG_12.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/BED_BUG_13.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/BED_BUG_13.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/BED_BUG_12.jpg\n",
      "Saved as: images/run20250806_203706/BED_BUG_13.jpg\n",
      "\n",
      "--- Processing post 23/25 ---\n",
      "Title: Bed Bugs?? What are these?...\n",
      "Comments: 1, Score: 0\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: mosquito\n",
      "Found 5 image(s)\n",
      "\n",
      "--- Processing post 23/25 ---\n",
      "Title: Bed Bugs?? What are these?...\n",
      "Comments: 1, Score: 0\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: mosquito\n",
      "Found 5 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_13.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_14.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_14.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/MOSQUITO_13.jpg\n",
      "Saved as: images/run20250806_203706/MOSQUITO_14.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_15.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_16.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_16.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/MOSQUITO_15.jpg\n",
      "Saved as: images/run20250806_203706/MOSQUITO_16.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_17.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/MOSQUITO_17.jpg\n",
      "\n",
      "--- Processing post 24/25 ---\n",
      "Title: What bug bit my 1 year old...\n",
      "Comments: 1, Score: 0\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: mosquito\n",
      "Found 3 image(s)\n",
      "\n",
      "--- Processing post 24/25 ---\n",
      "Title: What bug bit my 1 year old...\n",
      "Comments: 1, Score: 0\n",
      "Loaded 1 comments for analysis\n",
      "Comment upvotes: avg=1.0, max=1, total=1\n",
      "Detected bug type: mosquito\n",
      "Found 3 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_18.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_19.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_19.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_20.jpg\n",
      "INFO:__main__:Downloaded: images/run20250806_203706/MOSQUITO_20.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/MOSQUITO_18.jpg\n",
      "Saved as: images/run20250806_203706/MOSQUITO_19.jpg\n",
      "Saved as: images/run20250806_203706/MOSQUITO_20.jpg\n",
      "\n",
      "--- Processing post 25/25 ---\n",
      "Title: Possible black widow bite? Numb to touch...\n",
      "Comments: 0, Score: 0\n",
      "Loaded 0 comments for analysis\n",
      "Detected bug type: spider\n",
      "Found 1 image(s)\n",
      "\n",
      "--- Processing post 25/25 ---\n",
      "Title: Possible black widow bite? Numb to touch...\n",
      "Comments: 0, Score: 0\n",
      "Loaded 0 comments for analysis\n",
      "Detected bug type: spider\n",
      "Found 1 image(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: images/run20250806_203706/SPIDER_3.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: images/run20250806_203706/SPIDER_3.jpg\n",
      "Run metadata saved to: metadata/scraping_results_run20250806_203706.json\n",
      "Master metadata updated: metadata/all_scraping_results.json\n",
      "Run summary saved to: metadata/run_summary_20250806_203706.json\n",
      "\n",
      "======================================================================\n",
      "SCRAPING RUN COMPLETE!\n",
      "======================================================================\n",
      "Run timestamp: 20250806_203706\n",
      "Total images downloaded: 48\n",
      "Engagement Statistics:\n",
      "  Total comments analyzed: 240\n",
      "  Average engagement score: 11.8\n",
      "  Posts with comments: 47/48\n",
      "\n",
      "Images by bug type (this run):\n",
      "  ANT: 3 images (6.2%)\n",
      "  BED_BUG: 13 images (27.1%)\n",
      "  FLEA: 9 images (18.8%)\n",
      "  MOSQUITO: 20 images (41.7%)\n",
      "  SPIDER: 3 images (6.2%)\n",
      "\n",
      "âœ… Classification improvement! Only 0.0% unknown (down from previous runs)\n",
      "\n",
      "Run directory: images/run20250806_203706\n",
      "Files created (48 total):\n",
      "  ANT: 3 files\n",
      "    ANT_1.jpg\n",
      "    ANT_2.jpg\n",
      "    ANT_3.jpg\n",
      "  BED: 13 files\n",
      "    BED_BUG_1.jpg\n",
      "    BED_BUG_10.jpg\n",
      "    BED_BUG_11.jpg\n",
      "    ... and 10 more\n",
      "  FLEA: 9 files\n",
      "    FLEA_1.jpg\n",
      "    FLEA_2.jpg\n",
      "    FLEA_3.jpg\n",
      "    ... and 6 more\n",
      "  MOSQUITO: 20 files\n",
      "    MOSQUITO_1.jpg\n",
      "    MOSQUITO_10.jpg\n",
      "    MOSQUITO_11.jpg\n",
      "    ... and 17 more\n",
      "  SPIDER: 3 files\n",
      "    SPIDER_1.jpg\n",
      "    SPIDER_2.jpg\n",
      "    SPIDER_3.jpg\n",
      "\n",
      "Metadata files:\n",
      "  Run-specific: metadata/scraping_results_run20250806_203706.json\n",
      "  Master file: metadata/all_scraping_results.json\n",
      "\n",
      "Top posts by engagement in this run:\n",
      "  1. BED_BUG: Please help! Is this bed bugs...\n",
      "     Comments: 16, Score: 0, Engagement: 58\n",
      "  2. BED_BUG: Are these bed bug bites?...\n",
      "     Comments: 17, Score: 0, Engagement: 40\n",
      "  3. BED_BUG: Are these bed bug bites?...\n",
      "     Comments: 17, Score: 0, Engagement: 40\n",
      "\n",
      "Run completed at: 2025-08-06 20:38:00\n",
      "Next run suggestions:\n",
      "  - Try TIME_FILTER='week' for more recent discussions\n",
      "  - Try SORT_METHOD='controversial' for posts with more debate\n",
      "  - Try SORT_METHOD='hot' for currently trending posts\n",
      "Run metadata saved to: metadata/scraping_results_run20250806_203706.json\n",
      "Master metadata updated: metadata/all_scraping_results.json\n",
      "Run summary saved to: metadata/run_summary_20250806_203706.json\n",
      "\n",
      "======================================================================\n",
      "SCRAPING RUN COMPLETE!\n",
      "======================================================================\n",
      "Run timestamp: 20250806_203706\n",
      "Total images downloaded: 48\n",
      "Engagement Statistics:\n",
      "  Total comments analyzed: 240\n",
      "  Average engagement score: 11.8\n",
      "  Posts with comments: 47/48\n",
      "\n",
      "Images by bug type (this run):\n",
      "  ANT: 3 images (6.2%)\n",
      "  BED_BUG: 13 images (27.1%)\n",
      "  FLEA: 9 images (18.8%)\n",
      "  MOSQUITO: 20 images (41.7%)\n",
      "  SPIDER: 3 images (6.2%)\n",
      "\n",
      "âœ… Classification improvement! Only 0.0% unknown (down from previous runs)\n",
      "\n",
      "Run directory: images/run20250806_203706\n",
      "Files created (48 total):\n",
      "  ANT: 3 files\n",
      "    ANT_1.jpg\n",
      "    ANT_2.jpg\n",
      "    ANT_3.jpg\n",
      "  BED: 13 files\n",
      "    BED_BUG_1.jpg\n",
      "    BED_BUG_10.jpg\n",
      "    BED_BUG_11.jpg\n",
      "    ... and 10 more\n",
      "  FLEA: 9 files\n",
      "    FLEA_1.jpg\n",
      "    FLEA_2.jpg\n",
      "    FLEA_3.jpg\n",
      "    ... and 6 more\n",
      "  MOSQUITO: 20 files\n",
      "    MOSQUITO_1.jpg\n",
      "    MOSQUITO_10.jpg\n",
      "    MOSQUITO_11.jpg\n",
      "    ... and 17 more\n",
      "  SPIDER: 3 files\n",
      "    SPIDER_1.jpg\n",
      "    SPIDER_2.jpg\n",
      "    SPIDER_3.jpg\n",
      "\n",
      "Metadata files:\n",
      "  Run-specific: metadata/scraping_results_run20250806_203706.json\n",
      "  Master file: metadata/all_scraping_results.json\n",
      "\n",
      "Top posts by engagement in this run:\n",
      "  1. BED_BUG: Please help! Is this bed bugs...\n",
      "     Comments: 16, Score: 0, Engagement: 58\n",
      "  2. BED_BUG: Are these bed bug bites?...\n",
      "     Comments: 17, Score: 0, Engagement: 40\n",
      "  3. BED_BUG: Are these bed bug bites?...\n",
      "     Comments: 17, Score: 0, Engagement: 40\n",
      "\n",
      "Run completed at: 2025-08-06 20:38:00\n",
      "Next run suggestions:\n",
      "  - Try TIME_FILTER='week' for more recent discussions\n",
      "  - Try SORT_METHOD='controversial' for posts with more debate\n",
      "  - Try SORT_METHOD='hot' for currently trending posts\n"
     ]
    }
   ],
   "source": [
    "# Execute the scraping with enhanced post selection\n",
    "# Note: Make sure you've updated the Reddit API credentials above before running this!\n",
    "\n",
    "# Configure scraping parameters from environment variables\n",
    "POSTS_TO_SCRAPE = REDDIT_POSTS_COUNT  # Use count from .env file\n",
    "TIME_FILTER = 'month'  # Changed to 'month' to get more posts with established discussions\n",
    "\n",
    "# Map strategy names to sort methods and configurations\n",
    "STRATEGY_CONFIGS = {\n",
    "    'balanced': {'sort_method': 'top', 'time_filter': 'month'},\n",
    "    'discussion_heavy': {'sort_method': 'controversial', 'time_filter': 'all'},\n",
    "    'recent_active': {'sort_method': 'hot', 'time_filter': 'week'},\n",
    "    'controversial': {'sort_method': 'controversial', 'time_filter': 'month'},\n",
    "    'quality_focused': {'sort_method': 'top', 'time_filter': 'year'}\n",
    "}\n",
    "\n",
    "# Get configuration from strategy\n",
    "if REDDIT_POST_FETCH_STRATEGY in STRATEGY_CONFIGS:\n",
    "    strategy_config = STRATEGY_CONFIGS[REDDIT_POST_FETCH_STRATEGY]\n",
    "    SORT_METHOD = strategy_config['sort_method']\n",
    "    if REDDIT_POST_FETCH_STRATEGY in ['discussion_heavy', 'quality_focused']:\n",
    "        TIME_FILTER = strategy_config['time_filter']  # Override for specific strategies\n",
    "else:\n",
    "    SORT_METHOD = 'top'  # Default fallback\n",
    "    print(f\"âš ï¸ Unknown strategy '{REDDIT_POST_FETCH_STRATEGY}', using default 'top' sort\")\n",
    "\n",
    "# Sort method explanation:\n",
    "# 'top' - Most upvoted posts (good for quality content with discussions)\n",
    "# 'hot' - Currently trending (mix of new and popular)\n",
    "# 'new' - Newest posts (may have fewer comments)\n",
    "# 'controversial' - Posts with mixed reactions (often more discussion)\n",
    "# 'rising' - Posts gaining traction quickly\n",
    "\n",
    "print(\"Starting new Reddit scraping run with enhanced post selection...\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Target subreddit: r/{REDDIT_SUBREDDIT}\")\n",
    "print(f\"  Fetch strategy: {REDDIT_POST_FETCH_STRATEGY}\")\n",
    "print(f\"  Posts to scrape: {POSTS_TO_SCRAPE}\")\n",
    "print(f\"  Time filter: {TIME_FILTER}\")\n",
    "print(f\"  Sort method: {SORT_METHOD}\")\n",
    "print(f\"  Strategy: Prioritizing posts with more comments for better classification\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# List existing runs first\n",
    "print(\"\\nPrevious runs:\")\n",
    "list_all_runs()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STARTING NEW RUN WITH ENHANCED POST SELECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Run the scraper with new engagement-focused system\n",
    "scraped_data, run_timestamp = scrape_bugbites_subreddit(\n",
    "    limit=POSTS_TO_SCRAPE,\n",
    "    time_filter=TIME_FILTER,\n",
    "    sort_method=SORT_METHOD\n",
    ")\n",
    "\n",
    "# Save metadata with run information\n",
    "if scraped_data:\n",
    "    run_metadata_file, master_metadata_file = save_metadata_with_run(scraped_data, run_timestamp)\n",
    "\n",
    "    # Print detailed summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SCRAPING RUN COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Run timestamp: {run_timestamp}\")\n",
    "    print(f\"Total images downloaded: {len(scraped_data)}\")\n",
    "\n",
    "    # Calculate engagement statistics\n",
    "    total_comments_analyzed = sum(item.get('comments_analyzed', 0) for item in scraped_data)\n",
    "    avg_engagement = sum(item.get('engagement_score', 0) for item in scraped_data) / len(scraped_data)\n",
    "    posts_with_comments = sum(1 for item in scraped_data if item.get('comments_analyzed', 0) > 0)\n",
    "\n",
    "    print(f\"Engagement Statistics:\")\n",
    "    print(f\"  Total comments analyzed: {total_comments_analyzed}\")\n",
    "    print(f\"  Average engagement score: {avg_engagement:.1f}\")\n",
    "    print(f\"  Posts with comments: {posts_with_comments}/{len(scraped_data)}\")\n",
    "\n",
    "    # Count by bug type for this run\n",
    "    run_bug_counts = defaultdict(int)\n",
    "    unknown_count = 0\n",
    "    for item in scraped_data:\n",
    "        bug_type = item['bug_type']\n",
    "        run_bug_counts[bug_type] += 1\n",
    "        if bug_type == 'unknown':\n",
    "            unknown_count += 1\n",
    "\n",
    "    print(f\"\\nImages by bug type (this run):\")\n",
    "    for bug_type, count in sorted(run_bug_counts.items()):\n",
    "        percentage = (count / len(scraped_data)) * 100\n",
    "        print(f\"  {bug_type.upper()}: {count} images ({percentage:.1f}%)\")\n",
    "\n",
    "    # Show improvement\n",
    "    unknown_percentage = (unknown_count / len(scraped_data)) * 100\n",
    "    if unknown_percentage < 80:  # Arbitrary threshold\n",
    "        print(f\"\\nâœ… Classification improvement! Only {unknown_percentage:.1f}% unknown (down from previous runs)\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  Still {unknown_percentage:.1f}% unknown classifications\")\n",
    "\n",
    "    # Show run directory structure\n",
    "    print(f\"\\nRun directory: {current_run_dir}\")\n",
    "    try:\n",
    "        files = sorted([f for f in os.listdir(current_run_dir) if f.endswith('.jpg')])\n",
    "        print(f\"Files created ({len(files)} total):\")\n",
    "\n",
    "        # Group files by type for better display\n",
    "        files_by_type = defaultdict(list)\n",
    "        for f in files:\n",
    "            bug_type = f.split('_')[0]\n",
    "            files_by_type[bug_type].append(f)\n",
    "\n",
    "        for bug_type, type_files in sorted(files_by_type.items()):\n",
    "            print(f\"  {bug_type}: {len(type_files)} files\")\n",
    "            # Show first few files of each type\n",
    "            for f in type_files[:3]:\n",
    "                print(f\"    {f}\")\n",
    "            if len(type_files) > 3:\n",
    "                print(f\"    ... and {len(type_files) - 3} more\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing files: {e}\")\n",
    "\n",
    "    print(f\"\\nMetadata files:\")\n",
    "    print(f\"  Run-specific: {run_metadata_file}\")\n",
    "    print(f\"  Master file: {master_metadata_file}\")\n",
    "\n",
    "    # Show top posts by engagement\n",
    "    print(f\"\\nTop posts by engagement in this run:\")\n",
    "    sorted_data = sorted(scraped_data, key=lambda x: x.get('engagement_score', 0), reverse=True)\n",
    "    for i, item in enumerate(sorted_data[:3]):\n",
    "        print(f\"  {i+1}. {item['bug_type'].upper()}: {item['post_title'][:50]}...\")\n",
    "        print(f\"     Comments: {item.get('comments_analyzed', 0)}, \"\n",
    "              f\"Score: {item.get('post_score', 0)}, \"\n",
    "              f\"Engagement: {item.get('engagement_score', 0)}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo images were downloaded. Check your Reddit API credentials and internet connection.\")\n",
    "\n",
    "print(f\"\\nRun completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Next run suggestions:\")\n",
    "print(f\"  - Try TIME_FILTER='week' for more recent discussions\")\n",
    "print(f\"  - Try SORT_METHOD='controversial' for posts with more debate\")\n",
    "print(f\"  - Try SORT_METHOD='hot' for currently trending posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bfc4f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Strategy Presets Available:\n",
      "1. run_strategy_preset() - Uses strategy from .env file\n",
      "2. run_strategy_preset('balanced') - Recommended for general use\n",
      "3. run_strategy_preset('discussion_heavy') - For posts with lots of comments\n",
      "4. run_strategy_preset('recent_active') - For trending recent posts\n",
      "5. run_strategy_preset('controversial') - For posts with debates\n",
      "6. run_strategy_preset('quality_focused') - For highest quality posts\n",
      "\n",
      "Current .env settings:\n",
      "  Strategy: discussion_heavy\n",
      "  Posts count: 25\n",
      "\n",
      "Example usage:\n",
      "  data, timestamp = run_strategy_preset()  # Uses .env settings\n",
      "  data, timestamp = run_strategy_preset('discussion_heavy')  # Override strategy\n",
      "\n",
      "Or use the detailed configuration in the next cell.\n"
     ]
    }
   ],
   "source": [
    "# Quick Configuration Presets for Different Scraping Strategies\n",
    "\n",
    "def run_strategy_preset(strategy=None):\n",
    "    \"\"\"\n",
    "    Run predefined scraping strategies optimized for different goals\n",
    "    If no strategy provided, uses REDDIT_POST_FETCH_STRATEGY from .env\n",
    "\n",
    "    Strategies:\n",
    "    - 'balanced': Good mix of popular posts with comments (recommended)\n",
    "    - 'discussion_heavy': Focus on posts with lots of discussion\n",
    "    - 'recent_active': Recent posts that are gaining traction\n",
    "    - 'controversial': Posts with mixed reactions (often more detailed descriptions)\n",
    "    - 'quality_focused': Top-rated posts from longer time period\n",
    "    \"\"\"\n",
    "\n",
    "    # Use environment variable if no strategy specified\n",
    "    if strategy is None:\n",
    "        strategy = REDDIT_POST_FETCH_STRATEGY\n",
    "        print(f\"Using strategy from .env file: {strategy}\")\n",
    "\n",
    "    strategies = {\n",
    "        'balanced': {\n",
    "            'limit': REDDIT_POSTS_COUNT,\n",
    "            'time_filter': 'month',\n",
    "            'sort_method': 'top',\n",
    "            'description': 'Top posts from past month with good engagement'\n",
    "        },\n",
    "        'discussion_heavy': {\n",
    "            'limit': REDDIT_POSTS_COUNT,\n",
    "            'time_filter': 'all',\n",
    "            'sort_method': 'controversial',\n",
    "            'description': 'Controversial posts (all time) that generate discussion'\n",
    "        },\n",
    "        'recent_active': {\n",
    "            'limit': REDDIT_POSTS_COUNT,\n",
    "            'time_filter': 'week',\n",
    "            'sort_method': 'hot',\n",
    "            'description': 'Currently hot posts from past week'\n",
    "        },\n",
    "        'controversial': {\n",
    "            'limit': REDDIT_POSTS_COUNT,\n",
    "            'time_filter': 'month',\n",
    "            'sort_method': 'controversial',\n",
    "            'description': 'Most controversial posts from past month'\n",
    "        },\n",
    "        'quality_focused': {\n",
    "            'limit': REDDIT_POSTS_COUNT,\n",
    "            'time_filter': 'year',\n",
    "            'sort_method': 'top',\n",
    "            'description': 'Highest quality posts from past year'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if strategy not in strategies:\n",
    "        print(f\"Unknown strategy '{strategy}'. Available strategies:\")\n",
    "        for name, config in strategies.items():\n",
    "            print(f\"  {name}: {config['description']}\")\n",
    "        return\n",
    "\n",
    "    config = strategies[strategy]\n",
    "    print(f\"Running '{strategy}' strategy:\")\n",
    "    print(f\"  Target subreddit: r/{REDDIT_SUBREDDIT}\")\n",
    "    print(f\"  {config['description']}\")\n",
    "    print(f\"  Limit: {config['limit']}, Time: {config['time_filter']}, Sort: {config['sort_method']}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Run the scraper with this configuration\n",
    "    scraped_data, run_timestamp = scrape_bugbites_subreddit(\n",
    "        limit=config['limit'],\n",
    "        time_filter=config['time_filter'],\n",
    "        sort_method=config['sort_method']\n",
    "    )\n",
    "\n",
    "    # Save and analyze results\n",
    "    if scraped_data:\n",
    "        run_metadata_file, master_metadata_file = save_metadata_with_run(scraped_data, run_timestamp)\n",
    "\n",
    "        # Quick analysis\n",
    "        print(f\"\\nðŸŽ¯ Strategy '{strategy}' Results:\")\n",
    "        print(f\"   Downloaded: {len(scraped_data)} images\")\n",
    "\n",
    "        bug_counts = defaultdict(int)\n",
    "        for item in scraped_data:\n",
    "            bug_counts[item['bug_type']] += 1\n",
    "\n",
    "        unknown_pct = (bug_counts['unknown'] / len(scraped_data)) * 100 if scraped_data else 0\n",
    "        print(f\"   Unknown rate: {unknown_pct:.1f}%\")\n",
    "\n",
    "        if unknown_pct < 70:\n",
    "            print(\"   âœ… Good classification rate!\")\n",
    "        elif unknown_pct < 85:\n",
    "            print(\"   âš ï¸ Moderate classification rate\")\n",
    "        else:\n",
    "            print(\"   âŒ High unknown rate - try different strategy\")\n",
    "\n",
    "        return scraped_data, run_timestamp\n",
    "    else:\n",
    "        print(\"No data collected\")\n",
    "        return None, None\n",
    "\n",
    "# Quick preset runner - examples:\n",
    "print(\"Scraping Strategy Presets Available:\")\n",
    "print(\"1. run_strategy_preset() - Uses strategy from .env file\")\n",
    "print(\"2. run_strategy_preset('balanced') - Recommended for general use\")\n",
    "print(\"3. run_strategy_preset('discussion_heavy') - For posts with lots of comments\")\n",
    "print(\"4. run_strategy_preset('recent_active') - For trending recent posts\")\n",
    "print(\"5. run_strategy_preset('controversial') - For posts with debates\")\n",
    "print(\"6. run_strategy_preset('quality_focused') - For highest quality posts\")\n",
    "print()\n",
    "print(f\"Current .env settings:\")\n",
    "print(f\"  Strategy: {REDDIT_POST_FETCH_STRATEGY}\")\n",
    "print(f\"  Posts count: {REDDIT_POSTS_COUNT}\")\n",
    "print()\n",
    "print(\"Example usage:\")\n",
    "print(\"  data, timestamp = run_strategy_preset()  # Uses .env settings\")\n",
    "print(\"  data, timestamp = run_strategy_preset('discussion_heavy')  # Override strategy\")\n",
    "print()\n",
    "print(\"Or use the detailed configuration in the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67f7b816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Management Utilities Ready!\n",
      "\n",
      "Current runs:\n",
      "Found 7 previous runs:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Run 20250806_203706: 48 images, Sort: controversial, Comments: 240\n",
      "  Directory: images/run20250806_203706\n",
      "  Types: BED_BUG: 13, MOSQUITO: 20, ANT: 3, SPIDER: 3, FLEA: 9\n",
      "\n",
      "Run 20250806_203307: 36 images, Sort: top, Comments: 271\n",
      "  Directory: images/run20250806_203307\n",
      "  Types: MOSQUITO: 14, BED_BUG: 13, ANT: 1, TICK: 3, FLEA: 5\n",
      "\n",
      "Run 20250806_201819: 36 images, Sort: top, Comments: 271\n",
      "  Directory: images/run20250806_201819\n",
      "  Types: TICK: 5, BED_BUG: 16, DERMATITIS: 1, SCABIES: 1, SPIDER: 2, FLEA: 2, MITE: 4, BEE: 5\n",
      "\n",
      "Run 20250806_201209: 36 images, Sort: top, Comments: 271\n",
      "  Directory: images/run20250806_201209\n",
      "  Types: TICK: 1, UNKNOWN: 24, BED_BUG: 10, SCABIES: 1\n",
      "\n",
      "Run 20250806_200730: 36 images, Sort: top, Comments: 271\n",
      "  Directory: images/run20250806_200730\n",
      "  Types: TICK: 5, BED_BUG: 16, ANT: 1, MITE: 5, SPIDER: 2, FLEA: 2, BEE: 5\n",
      "\n",
      "Run 20250806_200212: 19 images, Sort: unknown, Comments: 0\n",
      "  Directory: images/run20250806_200212\n",
      "  Types: BED_BUG: 3, MOSQUITO: 8, UNKNOWN: 8\n",
      "\n",
      "Run 0: 19 images, Sort: unknown, Comments: 0\n",
      "  Directory: images/run0\n",
      "\n",
      "\n",
      "Analyzing most recent run:\n",
      "Found 7 previous runs:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Run 20250806_203706: 48 images, Sort: controversial, Comments: 240\n",
      "  Directory: images/run20250806_203706\n",
      "  Types: BED_BUG: 13, MOSQUITO: 20, ANT: 3, SPIDER: 3, FLEA: 9\n",
      "\n",
      "Run 20250806_203307: 36 images, Sort: top, Comments: 271\n",
      "  Directory: images/run20250806_203307\n",
      "  Types: MOSQUITO: 14, BED_BUG: 13, ANT: 1, TICK: 3, FLEA: 5\n",
      "\n",
      "Run 20250806_201819: 36 images, Sort: top, Comments: 271\n",
      "  Directory: images/run20250806_201819\n",
      "  Types: TICK: 5, BED_BUG: 16, DERMATITIS: 1, SCABIES: 1, SPIDER: 2, FLEA: 2, MITE: 4, BEE: 5\n",
      "\n",
      "Run 20250806_201209: 36 images, Sort: top, Comments: 271\n",
      "  Directory: images/run20250806_201209\n",
      "  Types: TICK: 1, UNKNOWN: 24, BED_BUG: 10, SCABIES: 1\n",
      "\n",
      "Run 20250806_200730: 36 images, Sort: top, Comments: 271\n",
      "  Directory: images/run20250806_200730\n",
      "  Types: TICK: 5, BED_BUG: 16, ANT: 1, MITE: 5, SPIDER: 2, FLEA: 2, BEE: 5\n",
      "\n",
      "Run 20250806_200212: 19 images, Sort: unknown, Comments: 0\n",
      "  Directory: images/run20250806_200212\n",
      "  Types: BED_BUG: 3, MOSQUITO: 8, UNKNOWN: 8\n",
      "\n",
      "Run 0: 19 images, Sort: unknown, Comments: 0\n",
      "  Directory: images/run0\n",
      "\n",
      "Analyzing most recent run: 20250806_203706\n",
      "\n",
      "Analysis of run 20250806_203706:\n",
      "==================================================\n",
      "Total images: 48\n",
      "Run directory: images/run20250806_203706\n",
      "Scraped at: 2025-08-06T20:37:07.841046\n",
      "\n",
      "Bug Type Distribution:\n",
      "  MOSQUITO: 20 images (41.7%)\n",
      "  BED_BUG: 13 images (27.1%)\n",
      "  FLEA: 9 images (18.8%)\n",
      "  ANT: 3 images (6.2%)\n",
      "  SPIDER: 3 images (6.2%)\n",
      "\n",
      "Average post score: 0.0\n",
      "Average comments per post: 5.9\n",
      "\n",
      "Sample classifications:\n",
      "  BED_BUG: Please help! Is this bed bugs...\n",
      "  BED_BUG: Are these bed bug bites?...\n",
      "  BED_BUG: Are these bed bug bites?...\n",
      "  BED_BUG: Are these bed bug bites?...\n",
      "  BED_BUG: Are these bed bug bites?...\n"
     ]
    }
   ],
   "source": [
    "# Run management and analysis utilities\n",
    "\n",
    "def analyze_run(timestamp=None):\n",
    "    \"\"\"Analyze a specific run or the most recent run\"\"\"\n",
    "\n",
    "    if timestamp is None:\n",
    "        # Find most recent run\n",
    "        runs = list_all_runs()\n",
    "        if not runs:\n",
    "            print(\"No runs found\")\n",
    "            return\n",
    "        timestamp = runs[0]['timestamp']\n",
    "        print(f\"Analyzing most recent run: {timestamp}\")\n",
    "\n",
    "    metadata_file = f'metadata/scraping_results_run{timestamp}.json'\n",
    "\n",
    "    try:\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        if not data:\n",
    "            print(\"No data found in run\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\nAnalysis of run {timestamp}:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Total images: {len(data)}\")\n",
    "        print(f\"Run directory: {data[0]['run_directory']}\")\n",
    "        print(f\"Scraped at: {data[0]['scraped_at']}\")\n",
    "\n",
    "        # Bug type distribution\n",
    "        bug_counts = defaultdict(int)\n",
    "        total_score = 0\n",
    "        total_comments = 0\n",
    "\n",
    "        for item in data:\n",
    "            bug_counts[item['bug_type']] += 1\n",
    "            total_score += item.get('post_score', 0)\n",
    "            total_comments += item.get('num_comments', 0)\n",
    "\n",
    "        print(f\"\\nBug Type Distribution:\")\n",
    "        for bug_type, count in sorted(bug_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            percentage = (count / len(data)) * 100\n",
    "            print(f\"  {bug_type.upper()}: {count} images ({percentage:.1f}%)\")\n",
    "\n",
    "        print(f\"\\nAverage post score: {total_score / len(data):.1f}\")\n",
    "        print(f\"Average comments per post: {total_comments / len(data):.1f}\")\n",
    "\n",
    "        # Show some sample classifications\n",
    "        print(f\"\\nSample classifications:\")\n",
    "        for i, item in enumerate(data[:5]):\n",
    "            print(f\"  {item['bug_type'].upper()}: {item['post_title'][:60]}...\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Run metadata not found: {metadata_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing run: {e}\")\n",
    "\n",
    "def compare_runs():\n",
    "    \"\"\"Compare all runs\"\"\"\n",
    "    runs = list_all_runs()\n",
    "\n",
    "    if len(runs) < 2:\n",
    "        print(\"Need at least 2 runs to compare\")\n",
    "        return\n",
    "\n",
    "    print(\"Run Comparison:\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Load all run data\n",
    "    all_run_data = []\n",
    "    for run in runs:\n",
    "        timestamp = run['timestamp']\n",
    "        metadata_file = f'metadata/scraping_results_run{timestamp}.json'\n",
    "\n",
    "        try:\n",
    "            with open(metadata_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            bug_counts = defaultdict(int)\n",
    "            for item in data:\n",
    "                bug_counts[item['bug_type']] += 1\n",
    "\n",
    "            all_run_data.append({\n",
    "                'timestamp': timestamp,\n",
    "                'total_images': len(data),\n",
    "                'bug_counts': dict(bug_counts)\n",
    "            })\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Print comparison table\n",
    "    print(f\"{'Run':<16} {'Total':<8} {'Unknown':<8} {'Mosquito':<10} {'Spider':<8} {'Others':<8}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for run_data in all_run_data:\n",
    "        timestamp = run_data['timestamp']\n",
    "        total = run_data['total_images']\n",
    "        unknown = run_data['bug_counts'].get('unknown', 0)\n",
    "        mosquito = run_data['bug_counts'].get('mosquito', 0)\n",
    "        spider = run_data['bug_counts'].get('spider', 0)\n",
    "        others = total - unknown - mosquito - spider\n",
    "\n",
    "        print(f\"{timestamp:<16} {total:<8} {unknown:<8} {mosquito:<10} {spider:<8} {others:<8}\")\n",
    "\n",
    "def cleanup_old_runs(keep_recent=5):\n",
    "    \"\"\"Clean up old runs, keeping only the most recent ones\"\"\"\n",
    "    runs = list_all_runs()\n",
    "\n",
    "    if len(runs) <= keep_recent:\n",
    "        print(f\"Only {len(runs)} runs found, nothing to clean up\")\n",
    "        return\n",
    "\n",
    "    runs_to_delete = runs[keep_recent:]\n",
    "\n",
    "    print(f\"Will delete {len(runs_to_delete)} old runs, keeping {keep_recent} most recent:\")\n",
    "    for run in runs_to_delete:\n",
    "        print(f\"  {run['timestamp']}\")\n",
    "\n",
    "    confirm = input(\"\\nProceed with deletion? (y/N): \")\n",
    "    if confirm.lower() != 'y':\n",
    "        print(\"Cleanup cancelled\")\n",
    "        return\n",
    "\n",
    "    deleted_count = 0\n",
    "    for run in runs_to_delete:\n",
    "        timestamp = run['timestamp']\n",
    "\n",
    "        # Delete run directory\n",
    "        run_dir = f\"images/run{timestamp}\"\n",
    "        if os.path.exists(run_dir):\n",
    "            import shutil\n",
    "            shutil.rmtree(run_dir)\n",
    "            deleted_count += 1\n",
    "\n",
    "        # Delete metadata files\n",
    "        for file_pattern in [f'metadata/scraping_results_run{timestamp}.json',\n",
    "                           f'metadata/run_summary_{timestamp}.json']:\n",
    "            if os.path.exists(file_pattern):\n",
    "                os.remove(file_pattern)\n",
    "\n",
    "    print(f\"Cleanup complete! Deleted {deleted_count} run directories\")\n",
    "\n",
    "# Quick analysis of existing data\n",
    "print(\"Run Management Utilities Ready!\")\n",
    "print(\"\\nCurrent runs:\")\n",
    "list_all_runs()\n",
    "\n",
    "if os.path.exists('images') and any(d.startswith('run') for d in os.listdir('images')):\n",
    "    print(\"\\nAnalyzing most recent run:\")\n",
    "    analyze_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e330c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No metadata file found yet. Run the scraper first!\n"
     ]
    }
   ],
   "source": [
    "# Analysis and utility functions\n",
    "def analyze_scraped_data(metadata_file='metadata/scraping_results.json'):\n",
    "    \"\"\"Analyze the scraped data and show statistics\"\"\"\n",
    "\n",
    "    try:\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        if not data:\n",
    "            print(\"No data found in metadata file\")\n",
    "            return\n",
    "\n",
    "        print(f\"Analysis of {len(data)} scraped images:\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        # Bug type distribution\n",
    "        bug_counts = defaultdict(int)\n",
    "        total_score = 0\n",
    "        total_comments = 0\n",
    "\n",
    "        for item in data:\n",
    "            bug_counts[item['bug_type']] += 1\n",
    "            total_score += item.get('post_score', 0)\n",
    "            total_comments += item.get('num_comments', 0)\n",
    "\n",
    "        print(\"\\nBug Type Distribution:\")\n",
    "        for bug_type, count in sorted(bug_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            percentage = (count / len(data)) * 100\n",
    "            print(f\"  {bug_type.upper()}: {count} images ({percentage:.1f}%)\")\n",
    "\n",
    "        print(f\"\\nAverage post score: {total_score / len(data):.1f}\")\n",
    "        print(f\"Average comments per post: {total_comments / len(data):.1f}\")\n",
    "\n",
    "        # Show some sample filenames\n",
    "        print(f\"\\nSample downloaded files:\")\n",
    "        for i, item in enumerate(data[:5]):\n",
    "            print(f\"  {item['filename']} - {item['bug_type']} - {item['post_title'][:50]}...\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Metadata file {metadata_file} not found. Run the scraper first!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing data: {e}\")\n",
    "\n",
    "def list_downloaded_images():\n",
    "    \"\"\"List all downloaded images\"\"\"\n",
    "    try:\n",
    "        images = [f for f in os.listdir('images') if f.endswith('.jpg')]\n",
    "        if images:\n",
    "            print(f\"Downloaded images ({len(images)} total):\")\n",
    "            for img in sorted(images):\n",
    "                print(f\"  {img}\")\n",
    "        else:\n",
    "            print(\"No images found in images/ directory\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Images directory not found\")\n",
    "\n",
    "# Run analysis if metadata exists\n",
    "if os.path.exists('metadata/scraping_results.json'):\n",
    "    analyze_scraped_data()\n",
    "else:\n",
    "    print(\"No metadata file found yet. Run the scraper first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db780e5",
   "metadata": {},
   "source": [
    "## Instructions for Use\n",
    "\n",
    "### Before Running the Scraper:\n",
    "\n",
    "1. **Set up Reddit API credentials:**\n",
    "   - Go to https://www.reddit.com/prefs/apps/\n",
    "   - Click \"Create App\" or \"Create Another App\"\n",
    "   - Choose \"script\" as the app type\n",
    "   - Fill in the required fields\n",
    "   - Copy your `client_id` and `client_secret`\n",
    "   - Update the credentials in the \"Reddit API Configuration\" cell above\n",
    "   - Optionally set `REDDIT_SUBREDDIT=your_target_subreddit` (defaults to 'bugbites')\n",
    "   - Optionally set `REDDIT_POST_FETCH_STRATEGY=strategy_name` (defaults to 'balanced')\n",
    "     - Available strategies: 'balanced', 'discussion_heavy', 'recent_active', 'controversial', 'quality_focused'\n",
    "   - Optionally set `REDDIT_POSTS_COUNT=number` (defaults to 15)\n",
    "\n",
    "2. **Configure scraping parameters:**\n",
    "   - Adjust `POSTS_TO_SCRAPE` (recommended: start with 10-25 for testing)\n",
    "   - Choose `TIME_FILTER` ('hour', 'day', 'week', 'month', 'year', 'all')\n",
    "\n",
    "### How the Script Works:\n",
    "\n",
    "1. **Connects to Reddit** using PRAW in read-only mode\n",
    "2. **Fetches posts** from r/bugbites subreddit\n",
    "3. **Analyzes text** (title + comments) to detect bug types using keyword matching\n",
    "4. **Downloads images** from posts and saves them as `BUGTYPE_N.jpg`\n",
    "5. **Saves metadata** about each image in JSON format\n",
    "\n",
    "### Output Structure:\n",
    "```\n",
    "images/\n",
    "â”œâ”€â”€ MOSQUITO_1.jpg\n",
    "â”œâ”€â”€ MOSQUITO_2.jpg\n",
    "â”œâ”€â”€ SPIDER_1.jpg\n",
    "â”œâ”€â”€ UNKNOWN_1.jpg\n",
    "â””â”€â”€ ...\n",
    "\n",
    "metadata/\n",
    "â””â”€â”€ scraping_results.json\n",
    "```\n",
    "\n",
    "### Troubleshooting:\n",
    "\n",
    "- **\"Error connecting to Reddit API\"**: Check your credentials\n",
    "- **\"No images found\"**: Some posts may not contain direct image links\n",
    "- **Rate limiting**: The script includes delays to respect Reddit's API limits\n",
    "- **Imgur albums**: Currently only handles single Imgur images, not full albums\n",
    "\n",
    "### Notes:\n",
    "\n",
    "- The script respects Reddit's API rate limits with built-in delays\n",
    "- Images are classified based on text analysis of titles and comments\n",
    "- Unknown or ambiguous posts are classified as 'UNKNOWN'\n",
    "- All metadata is preserved for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47010665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current BUG_TYPES['unknown'] category:\n",
      "['unknown', 'unidentified', 'mystery', 'unclear', 'unsure']\n",
      "\n",
      "This should only contain: ['unknown', 'unidentified', 'mystery', 'unclear', 'unsure']\n",
      "If it contains 'bug', 'bite', 'what bit', etc. - that's the problem!\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Check what's in the current BUG_TYPES['unknown'] category\n",
    "print(\"Current BUG_TYPES['unknown'] category:\")\n",
    "print(BUG_TYPES['unknown'])\n",
    "print()\n",
    "print(\"This should only contain: ['unknown', 'unidentified', 'mystery', 'unclear', 'unsure']\")\n",
    "print(\"If it contains 'bug', 'bite', 'what bit', etc. - that's the problem!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
